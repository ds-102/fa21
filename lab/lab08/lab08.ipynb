{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Bayesian and Frequentist Takes on Multi-Armed Bandits {-}\n",
    "Welcome to the eighth DS102 lab! \n",
    "\n",
    "The goals of this lab is to implement and gain a better understanding of the pros and cons of the Upper Confidence Bounds (UCB) and Thompson Sampling algorithms for the multi-armed bandits problem.\n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "In preparation for this lab we would recommend that you go over the [lecture notes](http://data102.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/ds-102/sp21&subPath=lecture/lecture18/bandits_notes.ipynb).\n",
    "\n",
    "## Collaboration Policy {-}\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "## Gradescope Submission {-}\n",
    "To submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Sunday, April 04, 2021 at 11:59 PM. PST**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators {-}\n",
    "Write the names of your collaborators in this cell.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from matplotlib.widgets import Button, CheckButtons\n",
    "from matplotlib import gridspec\n",
    "import functools\n",
    "from Bandit_env import BanditEnv, Interactive_UCB_Algorithm,Interactive_TS_Algorithm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Review - Concentration Inequalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Z = \\sum_{i=1}^{20} X_i$ where $X_1,\\ldots,X_{20}$ are i.i.d. Poisson random variables with parameter $\\lambda=20$. Use the following techniques to upper bound $\\mathbb{P}(X \\geq 26).$\n",
    "\n",
    "## 0a) Markov's Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0b) Chebyshev's Inequality\n",
    "\n",
    "Hint: Note that the question asks for an upper bound, so you should think about how the Chebyshev's Inequality can still be used here even though it is two-sided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0c) Chernoff Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits (MAB) {-}\n",
    "\n",
    "In this lab we will be implementing two of the most common approaches to solving stochastic Multi-Armed Bandit (MAB) problems. We first define the problem and then you will have a chance to implement the Upper Confidence Bound (UCB) algorithm and the Thompson Sampling (TS) algorithm from lecture and analyze their performance.\n",
    "\n",
    "Refer to Section 2 of the [lecture notes](http://data102.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/ds-102/sp21&subPath=lecture/lecture18/bandits_notes.ipynb) for all definitions. Note also that to help with book-keeping, we will adopt the expanded notation for rewards developed at the start of Section 6: We assume an $n\\times K$ matrix $X$ of *potential* rewards, where all entries are independent, and $X_{ij} \\sim P_j$ for each $i$ and $j$ (i.e. the entries in column $j$ are drawn from the reward distribution for arm $j$.) Note that we recover the old model by setting the observered reward at time $t$, $X_t$, to be equal to $X_{T_{A_t}(t),A_t}$ under this new notation. In other words if arm $a$ is pulled at step $t$, then the reward revealed is the earliest entry in the $a$-th column that has yet to be revealed thus far.\n",
    "\n",
    "### Lab setup: {-}\n",
    "In this lab, the MAB environment will have a set of arms numbered $0,1,...,K-1$. Each arm $a=0,1,...,K-1$ is associated with a Gaussian reward distribution with mean $\\mu_a$ and standard deviation of $\\sigma_a=1.5$. To be able to analyze the various algorithms, the optimal arm $a^\\ast$ will always be arm $0$, and its mean will always be $\\mu^\\ast=10$.\n",
    "\n",
    "By running the following cells, you can interact with a MAB instance of the type we will be using in this lab. You can see the reward distributions as well as the expected cumulative regret you incur when pulling each arm. \n",
    "\n",
    "Verify for yourself that explore-then-commit strategies can get stuck pulling the wrong arm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to initialize the parameters for the arms that we will be pulling from.\n",
    "\n",
    "# Mean reward for each arm. Arm 0 has the highest mean, but the algorithm doesn't know that yet.\n",
    "means=[10,9,8,7,6,4]\n",
    "\n",
    "# Variance of the reward for each arm.\n",
    "variance=1.5\n",
    "standard_deviations=[np.sqrt(variance) for arm in range(len(means))]\n",
    "\n",
    "# Initialize the interactive environment for pulling arms.\n",
    "bandit_env=BanditEnv(means,standard_deviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates an interactive bandit instance.\n",
    "\n",
    "    - Pull an arm by clickling on the colored button\n",
    "    - The true means of the distributions are shown with the dashed horizontal lines\n",
    "    - Large solid circle is the sample mean of the arm\n",
    "    - Small empty circle is a sample from the arm\n",
    "    - The reward distribution of each arm is shown on the right and can be toggled on/off by checking the box\n",
    "    - Running Pseudo-regret is shown on the bottom and can be toggled on/off by checking the box\n",
    "\"\"\"\n",
    "\n",
    "# You may need to rerun this cell to restart the gui\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize']=[7,6]\n",
    "bandit_env.run_Interactive()\n",
    "\n",
    "# You can click on the arms to see how it selecting sub-optimal arms accumulates regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.  The Frequentist Approach: Upper Confidence Bound (UCB) {-}\n",
    "\n",
    "The first algorithm we will analyze is a frequentist take on multi-armed bandits, known as the Upper Confidence Bound (UCB) algorithm. The algorithm was described in the lecture notes, but for convenience we reproduce the definition here, and adapt it to the setting of the lab.\n",
    "\n",
    "We make an optimistic forecast, called the **upper confidence bound**, for the mean of each arm. More precisely, we set\n",
    "\n",
    "$$\n",
    "\\text{UCB}_a(t,\\delta) = \\begin{cases}\n",
    "\\infty & \\quad \\text{if}~ T_a(t) = 0 \\\\\n",
    "\\hat{\\mu}_a(t) + \\sigma_a\\sqrt{\\frac{2\\log(1/\\delta)}{T_a(t)}} & \\quad \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "where\n",
    "$$\\hat{\\mu}_a(t) = \\frac{1}{T_a(t)}\\sum_{i=1}^{T_a(t)} X_{i,a}.$$\n",
    "\n",
    "The **UCB algorithm** is defined by iteratively making the arm selection:\n",
    "$$A_{t}= \\underset{a \\in \\{ 0,1,...,K-1\\}}{\\operatorname{argmax}} \\text{UCB}_a(t-1,\\delta_t).$$\n",
    "\n",
    "We set $\\delta_t = 1/t^3$.\n",
    "\n",
    "\n",
    "For each arm $a \\in \\{ 0,1,...,K-1\\}$, you hence need to keep track of:\n",
    "\n",
    "1. $T_a(t)$: the number of times arm $a$ has been pulled up to and including iteration $t$.\n",
    "2. $X_{1,a},\\ldots,X_{T_a(t),a}$: the samples you have received from arm $a$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Checkpoint\n",
    "\n",
    "(i) Explain why UCB starts by pulling every arm once.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Applying Hoeffding's inequality for Gaussian random variables, we have that for any $m=1,\\ldots, n$, we have\n",
    "\n",
    "$$ P( \\frac{1}{m}\\sum_{i=1}^m X_{i,a}-\\mu_a \\leq -\\epsilon) \\leq e^{-\\frac{m\\epsilon^2}{2\\sigma_a^2}}.$$\n",
    "\n",
    "Show that this results in the upper confidence bound on $\\mu_a$:\n",
    "\n",
    "$$P\\left(\\mu_a <\\frac{1}{m}\\sum_{i=1}^m X_{i,a} + \\sigma_a\\sqrt{\\frac{2\\log(1/\\delta)}{m}}\\right)> 1-\\delta. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. UCB Implementation\n",
    "\n",
    "**`TODO`: Now, use the formula for $A_t$ to fill out the following function which returns the choice of arm as well as the upper confidence bounds of each arm. In the code below, we use the variable \"confidence_bound\" to refer to the entire term $\\hat{\\mu}_a(t-1) + \\sigma_a\\sqrt{\\frac{2\\log(t^3)}{T_a(t-1)}}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def UCB_pull_arm(t, standard_deviations, times_pulled, rewards):\n",
    "    \"\"\" Implement the choice of arm for the UCB algorithm\n",
    "    \n",
    "    Inputs:\n",
    "        t : int, current iteration\n",
    "        standard_deviations : a list of length K (where K is the number of arms) of the \n",
    "            standard deviations associates with each arm\n",
    "        times_pulled: a list of length K (where K is the number of arms) of the number \n",
    "            of times each arm has been pulled.\n",
    "        rewards: a list of K lists. Each of the K lists holds the samples received from\n",
    "            pulling each arm up to iteration t. \n",
    "\n",
    "    Outputs:\n",
    "        arm: an integer representing the arm that the UCB algorithm would choose.\n",
    "        confidence_bounds: a list of the confidence bounds for each arm\n",
    "    \"\"\"\n",
    "\n",
    "    K = # TODO: fill in\n",
    "    delta = 1/(t**3)\n",
    "    \n",
    "    confidence_bounds=[]\n",
    "    for arm in range(K):\n",
    "        # TODO: fill in\n",
    "        \n",
    "        # Hint: the \\hat\\mu_a(t-1) value is the mean of rewards[arm], \n",
    "        # and the T_a(t-1) value is equal to times_pulled[arm].\n",
    "    \n",
    "    arm = # TODO: fill in\n",
    "    \n",
    "    return arm, confidence_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests, do not modify\n",
    "K_test = 5\n",
    "times_pulled_test = [3, 5, 7, 4, 0]\n",
    "t_test = np.sum(times_pulled_test) + 1\n",
    "standard_deviations_test = [0.4, 0.2, 0.1, 0.2, 0.5]\n",
    "rewards_test = [[10.4, 10.6, 11], \n",
    "                [8, 13, 12, 11, 9], \n",
    "                [9, 10, 10, 8, 9.5, 10.5, 11],\n",
    "                [8.3, 9.6, 7.9, 8.1],\n",
    "                []]\n",
    "test_arm, test_confidence_bounds = UCB_pull_arm(t_test, standard_deviations_test, times_pulled_test, rewards_test)\n",
    "assert test_arm == 4\n",
    "assert np.isinf(test_confidence_bounds[-1])\n",
    "opt_vals = [11.64, 10.98, 9.87, 8.90]\n",
    "for a in range(K_test-1):\n",
    "    assert (np.abs(opt_vals[a] - test_confidence_bounds[a]) <= 0.1)\n",
    "print(\"Test Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function you have filled out, let us investigate the pseudo-regret of the UCB algorithm.  Since the pseudo-regret is an expectation of the regret, the following cell runs the algorithm $20$ times and computes the average pseudo-regret across all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "# Define the time horizon of each run, and the number of runs of each the algorithm.\n",
    "T=1000\n",
    "num_runs=20\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "UCB_pseudo_regret=0\n",
    "for runs in range(num_runs):\n",
    "    #Initialize Bandit_environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Choose arm using UCB algorithm\n",
    "        arm,confidence_bounds=UCB_pull_arm(t, standard_deviations, bandit_env.times_pulled,bandit_env.rewards)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "        \n",
    "    #Keep track of pseudo-regret  \n",
    "    UCB_pseudo_regret+=np.array(bandit_env.regret)\n",
    "    \n",
    "#Make plot\n",
    "plt.plot(UCB_pseudo_regret/num_runs) \n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pseudo-Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Your Algorithm{-}\n",
    "If you want to visualize your algorithm, you can use the following interactive demo (If it is lagging, do not worry -- this part is not graded and is just meant to build your intuition for the algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[9,8]\n",
    "\n",
    "\"\"\"\n",
    "Creates an interactive bandit instance with an option to test your algorithm.\n",
    "    - Pull an arm by clickling on the colored button\n",
    "    - Allow your algorithm to choose the arm by clicking on the ``UCB'' button in the lower right.\n",
    "    - The true means of the distributions are shown with the dashed horizontal lines\n",
    "    - Large solid circle is the sample mean of the arm\n",
    "    - Solid vertical line is the upper confidence bound you have calculated\n",
    "    - The reward distribution of each arm is shown on the right and can be toggled on/off by checking the box\n",
    "    - Running Pseudo-regret is shown on the bottom and can be toggled on/off by checking the box\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# You may need to rerun this cell to restart the gui\n",
    "alg=Interactive_UCB_Algorithm(bandit_env,UCB_pull_arm,'UCB')\n",
    "alg.run_Interactive_Alg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. The Bayesian Approach: Thompson Sampling {-}\n",
    "\n",
    "Now we will analyze is the Bayesian take on multi-armed bandits, known as Thompson Sampling. We begin with a prior over the mean of each arm $\\mu_a \\sim \\pi_a$. The algorithm is described below:\n",
    "\n",
    "**Thompson sampling algorithm**: At each round $t$\n",
    "1. Draw a posterior sample for each arm: $\\mu_{a,t} \\sim  P_{a,t}$ for $a \\in \\{1,...,K\\}$.\n",
    "2. Choose the arm with the largest sample:\n",
    "$ A_t=\\underset{0 \\leq a \\leq K-1}{\\operatorname{argmax}} \\mu_{a,t}$\n",
    "\n",
    "In other words, the probability we assign to choosing arm $a$ is the probability that it is the largest arm under the posterior on all of the data.\n",
    "\n",
    "Since the reward distributions in this lab are Gaussians with known variance $\\sigma_a^2$, we know from our investigation of conjugate priors that if we have Gaussian priors: $\\mu_a\\sim\\mathcal{N}(\\mu_{a,0},\\sigma_{a,0}^2)$, and Gaussian likelihoods $X_a^{(i)}|\\mu_a \\sim \\mathcal{N}(\\mu_a, \\sigma_a)$ the posterior distribution for each arm will also be a Gaussian. \n",
    "\n",
    "Therefore, to implement Thompson Sampling in this lab, the posterior distributions for each arm in this lab at each time $t=1,2,...$ are given by:\n",
    "\n",
    "$$ P_{a,t}=\\mathcal{N}(\\hat\\mu_{a,t},\\hat{\\sigma}_{a,t}^2)$$\n",
    "\n",
    "where,\n",
    "$$\\hat{\\sigma}_{a,t}^2 =\\bigg(\\frac{1}{\\sigma_{a,0}^2}+\\frac{T_a(t-1)}{\\sigma_a^2}\\bigg)^{-1} $$\n",
    "$$ \\hat\\mu_{a,t}=\\hat{\\sigma}_{a,t}^2 \\bigg( \\frac{\\mu_{a,0}}{\\sigma_{a,0}^2}+\\frac{\\sum_{i=1}^{T_a(t-1)} X_{a,i}}{\\sigma_a^2} \\bigg)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a Checkpoint\n",
    "\n",
    "What are the formulas for the posterior mean and variance when we use an improper prior $\\mu_{a,0} = 0$ and $\\sigma_{a,0}^2 = \\infty$?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b Implementing Thompson Sampling {-}\n",
    "\n",
    "`TODO:` **Fill out the following function that implements the choice of arm for the Thompson Sampling algorithm with Gaussian arms and prior.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def TS_pull_arm(t,variances,times_pulled,rewards,prior_means,prior_variances):\n",
    "    \"\"\" \n",
    "    Implement the choice of arm for the Thompson Sampling Algorithm when the arms and priors are Gaussians.\n",
    "    \n",
    "    Inputs:\n",
    "        t: int, number of iteration of the bandit algorithm.\n",
    "        variances: a list of length K (where K is the number of arms) of the variances\n",
    "            corresponding to each arm (\\sigma_a^2 in the likelihood expression above)\n",
    "        times_pulled: a list of length K of the number of times each arm has been pulled.\n",
    "        rewards: a list of K lists. Each of the K lists holds the samples received from pulling each arm up \n",
    "            to iteration t.\n",
    "        prior_means: a list of length K with the mean of the priorsfor each arm.\n",
    "        prior_mea: a list of length K with the variance of the prior for each arm.\n",
    "    \n",
    "    Outputs:\n",
    "        arm: integer representing the arm that the TS algorithm would choose.\n",
    "        posterior_samples: list of samples from the posterior used to choose the arm. \n",
    "        posterior_means: list of means of the posterior for each arm\n",
    "        posterior_vars: list of variances of the posteriors of each arm\n",
    "    \"\"\"\n",
    "\n",
    "    K= # TODO: fill in\n",
    "    \n",
    "    posterior_samples=[]\n",
    "    posterior_means=[]\n",
    "    posterior_vars=[]\n",
    "    for arm in range(K):\n",
    "        # Hint: \\hat\\sigma^2_{a,0} is prior_variances[arm], \\sigma^2_a is variance,\n",
    "        # and T_a(t-1) is times_pulled[arm] (as before).\n",
    "        # Hint: \\mu_{a,0} is prior_means[arm], and X_a^{(i)} is rewards[arm] (as before).\n",
    "            \n",
    "    arm= # TODO: fill in\n",
    "    \n",
    "    return arm, posterior_samples, posterior_means, posterior_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests: Do not modify\n",
    "K_test = 5\n",
    "times_pulled_test = [3, 5, 7, 4, 1]\n",
    "prior_means_test=[8,5,7,9,6]\n",
    "prior_variances_test=[2.5, 0.1, 1.6, 2.3, 1.7]\n",
    "t_test = np.sum(times_pulled_test) + 1\n",
    "variances_test = [0.4, 0.2, 0.1, 0.2, 0.5]\n",
    "rewards_test = [[10.4, 12.6, 11], \n",
    "                [8, 13, 12, 11, 9], \n",
    "                [9, 10, 10, 8, 9.5, 10.5, 11],\n",
    "                [8.3, 9.6, 7.9, 8.1],\n",
    "                [8]]\n",
    "test_arm, posterior_samples_test, posterior_means_test, posterior_vars_test = TS_pull_arm(t_test, \n",
    "                                                                                          variances_test, \n",
    "                                                                                          times_pulled_test, \n",
    "                                                                                          rewards_test,\n",
    "                                                                                          prior_means_test,\n",
    "                                                                                          prior_variances_test)\n",
    "assert test_arm == 0\n",
    "opt_vals_means = [11.16, 9.0, 9.69, 8.49, 7.55]\n",
    "opt_vals_vars = [0.123, 0.0286, 0.014,0.049,0.386]\n",
    "for a in range(K_test):\n",
    "    assert (np.abs(opt_vals_means[a] - posterior_means_test[a]) <= 0.1)\n",
    "    assert (np.abs(opt_vals_vars[a] - posterior_vars_test[a]) <= 0.01)\n",
    "print(\"Test Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling with Good Priors {-}\n",
    "As we saw in class, the performance of Thompson Sampling can vary drastically with the quality of the prior. \n",
    "\n",
    "First, let us analyze the performance of Thompson Sampling when the priors reflect the correct rankings of the arms (meaning that the prior mean for arm $0$ is the highest). We will compare it to the performance of the UCB algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run the code and inspect the plot\n",
    "#Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "# Variance of the reward for each arm.\n",
    "variance=1.5\n",
    "true_variances=[variance for arm in range(len(means))]\n",
    "\n",
    "#Define Prior Means and Variances\n",
    "prior_means=[12,9,8,7,4,3]\n",
    "prior_vars=[3.2,3.2,3.2,3.2,3.2,3.2]\n",
    "\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "TS_pseudo_regret=0\n",
    "\n",
    "for runs in range(num_runs):\n",
    "    #Initialize bandit environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Choose arm with Thompson Sampling\n",
    "        arm,samples,means,variances=TS_pull_arm(t,true_variances,bandit_env.times_pulled,bandit_env.rewards,prior_means,prior_vars)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #Keep track of regret Regret\n",
    "    TS_pseudo_regret+=np.array(bandit_env.regret)\n",
    "    \n",
    "#Plot Thompson Sampling vs. UCB regret\n",
    "plt.plot(TS_pseudo_regret/num_runs ,label='TS Regret')\n",
    "plt.plot(UCB_pseudo_regret/num_runs ,label='UCB Regret')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pseudo-Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling with Bad Priors {-}\n",
    "Now let us analyze the performance of Thompson Sampling when the priors have completely incorrect correct rankings of the arms, meaning that the prior mean for arm $0$ is the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run the code and inspect the plot\n",
    "#Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "#Define prior means and standard deviations\n",
    "prior_means=[2,3,4,5,6,7]\n",
    "prior_vars=[3.2,3.2,3.2,3.2,3.2,3.2]\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "TS_pseudo_regret=0\n",
    "for runs in range(num_runs):\n",
    "    #Initialize bandit environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Chosoe arm with Thompson Sampling\n",
    "        arm,samples,means,variances=TS_pull_arm(t,true_variances,bandit_env.times_pulled,bandit_env.rewards,prior_means,prior_vars)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #Keep track of regret Regret\n",
    "    TS_pseudo_regret+=np.array(bandit_env.regret)\n",
    "    \n",
    "#Plot Thompson Sampling vs. UCB regret\n",
    "plt.plot(TS_pseudo_regret/num_runs ,label='TS Regret')\n",
    "plt.plot(UCB_pseudo_regret/num_runs ,label='UCB Regret')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pseudo-Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling with the same prior for each arm {-}\n",
    "Now let us analyze the performance of Thompson Sampling when the priors are the same for all arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run the code and inspect the plot\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "#Define prior means and variances\n",
    "prior_means=[8,8,8,8,8,8]\n",
    "prior_vars=[2.5,2.5,2.5,2.5,2.5,2.5]\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "TS_pseudo_regret=0\n",
    "for runs in range(num_runs):\n",
    "    #Initialize bandit environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Chosoe arm with Thompson Sampling\n",
    "        arm,samples,means,variances=TS_pull_arm(t,true_variances,bandit_env.times_pulled,bandit_env.rewards,prior_means,prior_vars)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #Keep track of regret Regret\n",
    "    TS_pseudo_regret+=np.array(bandit_env.regret)\n",
    "    \n",
    "#Plot Thompson Sampling vs. UCB regret\n",
    "plt.plot(TS_pseudo_regret/num_runs ,label='TS Regret')\n",
    "plt.plot(UCB_pseudo_regret/num_runs ,label='UCB Regret')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pseudo-Regret')\n",
    "plt.show()\n",
    "# You may need to rerun this cell to restart the gui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Your Algorithm {-}\n",
    "If you want to visualize your algorithm, you can use the following interactive demo (If it is lagging, do not worry this part is not graded and is meant to build your intuition for the algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[9,8]\n",
    "\n",
    "\"\"\"\n",
    "Creates an interactive bandit instance with an option to test your algorithm.\n",
    "    - Pull an arm by clickling on the colored button.\n",
    "    - Allow your algorithm to choose the arm by clicking on the ``TS'' button in the lower right.\n",
    "    - The true means of the distributions are shown with the dashed horizontal lines.\n",
    "    - Large solid circle is the sample mean of the rewards for the arm.\n",
    "    - Solid vertical line shows the 95% credible interval for the arm.\n",
    "    - The reward distribution of each arm is shown on the right and can be toggled on/off by checking the box.\n",
    "    - Running Pseudo-regret is shown on the bottom and can be toggled on/off by checking the box.\n",
    "\"\"\"\n",
    "\n",
    "#Define prior means and variances\n",
    "prior_means=[8,8,8,8,8,8]\n",
    "prior_vars=[2.5,2.5,2.5,2.5,2.5,2.5]\n",
    "\n",
    "# You may need to rerun this cell to restart the gui\n",
    "alg=Interactive_TS_Algorithm(bandit_env,TS_pull_arm,'TS',prior_means,prior_vars)\n",
    "alg.run_Interactive_Alg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Pros and Cons of UCB and Thompson Sampling {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, write a few sentences comparing and contrasting UCB and Thompson Sampling. What are some pros and cons of UCB and of Thompson Sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('husky.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.axes.get_xaxis().set_visible(False)\n",
    "imgplot.axes.get_yaxis().set_visible(False)\n",
    "print(\"Yay, you've made it to the end of Lab 8!\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
