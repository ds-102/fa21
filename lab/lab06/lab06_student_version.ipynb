{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Nonparametric methods\n",
    "\n",
    "Welcome to the sixth DS102 lab! \n",
    "\n",
    "The goal of this lab is to explore and interpret several nonparametric methods for regression.\n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "In preparation for this lab we would recommend that you review the slides and demos from Lectures 14 and 15.\n",
    "\n",
    "## Collaboration Policy {-}\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "## Gradescope Submission {-}\n",
    "\n",
    "To submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Wednesday, October 20, 2021 at 11:59 PM. PST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators {-}\n",
    "Write the names of your collaborators in this cell.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import hashlib\n",
    "def get_hash(num, significance = 4):\n",
    "    num = round(num, significance)\n",
    "    \"\"\"Helper function for assessing correctness\"\"\"\n",
    "    return hashlib.md5(str(num).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model comparison\n",
    "\n",
    "In this lab, we'll be working with the hybrid car dataset ([which you may remember from Data 8](https://inferentialthinking.com/chapters/15/1/Correlation.html)).\n",
    "\n",
    "It contains data on 153 different models of hybrid car from 1997 to 2013, with the price (`msrp`), gas mileage (`mpg`), type of car (`class`), and how fast the car accelerates in km/hour/second (`acceleration`).\n",
    "\n",
    "We're going to try to predict the price using other features of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid = pd.read_csv('hybrid.csv')\n",
    "\n",
    "X_cols = [\"year\", \"acceleration\", \"mpg\"] # Columns used for prediction\n",
    "y_col = \"msrp\" # The column we're trying to predict\n",
    "\n",
    "hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates all pairs of scatterplots for numerical variables in the data. You should see the same trends discussed in the chapter of the Data 8 textbook linked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a) Splitting the data\n",
    "\n",
    "We'll start by splitting the data into training and test sets. Use the scikit-learn function `train_test_split` to make two dataframes called `train` and `test`. The test set should have $30\\%$ of the data (46 rows).\n",
    "\n",
    "The `train_test_split` function has an argument called `random_state` that lets you ensure that it uses the same random split every time: you should set that argument to `102` to pass the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: fill in\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = ... # TODO: fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test\n",
    "assert(get_hash(train) == 'a8821bb31a8ae0d2b7803004be63656d')\n",
    "assert(get_hash(test) == '96dbf091169d3503cd962bbba32db3f3')\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b) Predicting the output\n",
    "\n",
    "#### 1.b.(i) Linear regression\n",
    "\n",
    "Use linear regression to predict the MSRP from year, acceleration, and MPG. Add a new column to the `train` and `test` dataframes called `linear_pred` with the predictions from linear regression.\n",
    "\n",
    "*Hint: throughout this lab, you should use the default values of all parameters for all models we're experimenting with.*\n",
    "\n",
    "*Hint: for this lab, you don't need to worry about pandas warnings about setting a value on a copy of a slice.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "X = ... # TODO: fill in\n",
    "y = ... # TODO: fill in\n",
    "... # TODO: fill in\n",
    "\n",
    "train[\"linear_pred\"] = ... # TODO: fill in\n",
    "test[\"linear_pred\"] = ... # TODO: fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test\n",
    "assert(get_hash(train[\"linear_pred\"]) == '42969b6301fad9e5e98680924754e9d6')\n",
    "assert(get_hash(test[\"linear_pred\"]) == '104e14d773c8efc0099711f9e62f0123')\n",
    "assert(isinstance(linear_model, LinearRegression))\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell that computes the training set error and test set error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = np.mean((train[\"linear_pred\"] - train[\"msrp\"]) ** 2) ** 0.5\n",
    "test_rmse = np.mean((test[\"linear_pred\"] - test[\"msrp\"]) ** 2) ** 0.5\n",
    "\n",
    "print(\"Training set error for linear model:\", train_rmse)\n",
    "print(\"Test set error for linear model:    \", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b.(ii) Decision Trees\n",
    "\n",
    "Recall that a decision tree is a method for classification and regression that uses a tree-like structure to decide what value to predict for a point.\n",
    "\n",
    "\n",
    "In this question, we'll use a decision tree for regression instead of classification. When we built a decision tree for classification in lecture, we made decisions about splitting based on how homogeneous the $y$-values were. Now, we'll instead make splits based on the residuals for predicting at that node. \n",
    "\n",
    "Let's look at an example, assuming we're using mean squared error as our loss. For example, if we make our first split based on whether or not `mpg <= M`, we'll have some average MSRP for the low-MPG cars (below `M`), along with residuals if we used that average to predict the MSRP for all the low-MPG cars. Similarly, we have the same information for the high-MPG cars (above `M`). A good value of `M` will make the mean squared residuals for the two groups as small as possible. So, at each node, we choose a split that makes the mean squared error on each side as small as possible.\n",
    "\n",
    "Compute the prediction from a decision tree with the default parameters for scikit-learn's `DecisionTreeRegressor` (i.e., no limit on tree depth). Add a new column to the `train` and `test` dataframes called `tree_pred` with the predictions from the decision tree.\n",
    "\n",
    "*Hint: your code should look very similar to your answer from 1.b.(i).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_model = ... # TODO: fill in\n",
    "... # TODO: fill in\n",
    "\n",
    "train[\"tree_pred\"] = ... # TODO: fill in\n",
    "test[\"tree_pred\"] = ... # TODO: fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test\n",
    "assert(get_hash(train[\"tree_pred\"]) == 'dbcb9fedf8018825bf704351b0cd7f9d')\n",
    "assert(isinstance(tree_model, DecisionTreeRegressor))\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell that computes the training set error and test set error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = np.mean((train[\"tree_pred\"] - train[\"msrp\"]) ** 2) ** 0.5\n",
    "test_rmse = np.mean((test[\"tree_pred\"] - test[\"msrp\"]) ** 2) ** 0.5\n",
    "\n",
    "print(\"Training set error for decision tree:\", train_rmse)\n",
    "print(\"Test set error for decision tree:    \", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b.(iii) Random Forest\n",
    "\n",
    "Recall that a random forest is the combination of a large number of decision trees.\n",
    "\n",
    "Compute the prediction from a decision tree using scikit-learn's `RandomForestRegressor`, with the following parameters:\n",
    "* 100 trees (default)\n",
    "* no limit on each tree's depth (default)\n",
    "* Use the `max_features` parameter to only use one feature for each tree. (*The recommended value for random forests is for each tree to only use 1/3 of the features, and in this case we have 3 features.*)\n",
    "\n",
    "Add a new column to the `train` and `test` dataframes called `forest_pred` with the predictions from the random forest.\n",
    "\n",
    "*Hint: your code should look very similar to your answers from 1.b.(i) and 1.b.(ii).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_model = ... # TODO: fill in\n",
    "... # TODO: fill in\n",
    "\n",
    "train[\"forest_pred\"] = ... # TODO: fill in\n",
    "test[\"forest_pred\"] = ... # TODO: fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test: If you aren't passing these, your random forest is doing something wrong\n",
    "assert((((train[\"forest_pred\"] - train[\"msrp\"]) **2).sum()) < 3981470000.0)\n",
    "assert((((test[\"forest_pred\"] - test[\"msrp\"]) **2).sum()) < 8015040000.0)\n",
    "assert(isinstance(forest_model, RandomForestRegressor))\n",
    "assert(forest_model.max_features != 'auto')\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell that computes the training set error and test set error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = np.mean((train[\"forest_pred\"] - train[\"msrp\"]) ** 2) ** 0.5\n",
    "test_rmse = np.mean((test[\"forest_pred\"] - test[\"msrp\"]) ** 2) ** 0.5\n",
    "\n",
    "print(\"Training set error for random forest:\", train_rmse)\n",
    "print(\"Test set error for random forest:    \", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c) accuracy comparison\n",
    "\n",
    "Of the Decision Tree model and the Random Forest model, which one does best on the training set? Why? Which model does best on the test set? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d Interpretability\n",
    "\n",
    "#### 1.d.i Linear Regression\n",
    "\n",
    "Let's look at the coefficients from the linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can just run this cell to print out the coefficients for each feature:\n",
    "print(X_cols)\n",
    "linear_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this result, fill in the blanks in the following two statements:\n",
    "\n",
    "\"Each year, linear regression predicts that the average price changes by $\\$ \\rule{1cm}{0.15mm}$ \". (Your answer should be a positive or negative number)\n",
    "\n",
    "\"Linear regression predicts that cars with better gas mileage are $\\rule{1cm}{0.15mm}$ expensive.\" (Your answer should be either 'more' or 'less')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.d.ii Decision trees\n",
    "\n",
    "We'll use the `plot_tree` function to draw the decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(16, 4))\n",
    "plot_tree(tree_model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tree is quite deep and complex. Let's take a closer look at the nodes at the top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plot_tree(tree_model, max_depth=2, fontsize=14, feature_names=X_cols);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things we can see right away:\n",
    "\n",
    "\n",
    "* The first line tells us which feature to split on: values below the threshold go to the left, and values above go to the right.\n",
    "* The third line tells us the number of training samples that made it that far into the tree.\n",
    "* The fourth line tells us the average $y$-value (in this case, MSRP) of all the training samples that made it that far into the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at the first few layers, we can already see that the decision tree has pulled out the most expensive cars into some of the branches, and the less expensive ones into other branches.\n",
    "\n",
    "Suppose we had stopped growing the tree at this point. That would have given us four leaf nodes, each with very different mean MSRP. Describe the node that contains the most expensive cars in plain English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Your answer here:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "Unfortunately, random forests are much harder to interpret than either of the other two methods that we've tried. In this case, with so few features, we might be able to look at the top of each tree and find similarities across most or all of the trees, but in high-dimensional problems, each tree should see a very different subset of features, and this becomes much harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many methods for explainable ML use the following setup to explain a specific prediction from a complex model:\n",
    "\n",
    "1. Construct a simpler, easier-to-explain model (e.g., linear regression, decision tree, etc.) that behaves similarly to the complex model for data points near the specific point we're trying to explain.\n",
    "2. Interpret the simpler model.\n",
    "\n",
    "In this question, we'll try to see if we can come up with an explanation for the worst predictions from each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a.i Finding the worst predictions\n",
    "\n",
    "Find the two cars in the test set where linear regression does the worst (i.e., has the highest absolute error, not the squared error). Your answer should be a dataframe with two rows from `test`, one for each of the two cars. You can add extra columns to `test` if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: There are a couple of ways to do this. One suggestion is to first create new column with the \n",
    "# absolute linear error and then sort the dataframe based on this absolute linear error.\n",
    "\n",
    "\n",
    "worst_linear_predicted_cars_df = ... # TODO: fill in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test\n",
    "assert(get_hash(worst_linear_predicted_cars_df.index.values[0]) == '6ea9ab1baa0efb9e19094440c317e21b')\n",
    "assert(get_hash(worst_linear_predicted_cars_df.index.values[1]) == 'd09bf41544a3365a46c9077ebb5e35c3')\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a.ii Explanation\n",
    "\n",
    "Using the coefficients of the linear model that we found earlier, explain why linear regression's predictions for these two cars were the way they were. Is the explanation from the linear model consistent with the trends you observed at the beginning in the visualizations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Your answer here:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b.i Finding the worst predictions for the random forest\n",
    "\n",
    "Find the two cars in the test set where random forest does the worst (i.e., has the highest absolute error). Your answer should be a dataframe with two rows from `test`, one for each of the two cars. You can add extra columns to `test` if you wish.\n",
    "\n",
    "*Hint: your code should be very similar to your code for 2.a.i.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_forest_predicted_cars_df = ...# TODO: fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test\n",
    "assert(get_hash(worst_forest_predicted_cars_df.index.values[0]) == '6ea9ab1baa0efb9e19094440c317e21b')\n",
    "assert(get_hash(worst_forest_predicted_cars_df.index.values[1]) == 'd09bf41544a3365a46c9077ebb5e35c3')\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Engineering and Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will be exploring the effect of feature engineering on the interpretability of a given model using a toy dataset. Let's start by loading the data, which has already been split for you into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same plotting function from lecture\n",
    "\n",
    "def draw_results(x1, x2, color, plot_title=''):\n",
    "    plt.figure()\n",
    "    plt.scatter(x1, x2, c=color, cmap='viridis', alpha=0.7);\n",
    "    plt.colorbar()\n",
    "    plt.title(plot_title)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "ring_train = pd.read_csv('ring_train.csv')\n",
    "ring_test = pd.read_csv('ring_test.csv')\n",
    "draw_results(ring_train['x1'], ring_train['x2'], color=ring_train['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from lecture that without any additional features, logistic regression will use a line as a decision boundary. \n",
    "Where would you draw the best line to classify these points? (No need to answer, but please think about it.)\n",
    "\n",
    "We are now going to fit a simple logistic regression model on the data using the following lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to write any code here: just understand.\n",
    "X_train = ring_train[['x1', 'x2']].values\n",
    "y_train = ring_train['y'].values\n",
    "\n",
    "X_test = ring_test[['x1', 'x2']].values\n",
    "y_test = ring_test['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to write any code here: just understand.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = ring_train[['x1', 'x2']].values\n",
    "y_train = ring_train['y'].values\n",
    "\n",
    "X_test = ring_test[['x1', 'x2']].values\n",
    "y_test = ring_test['y'].values\n",
    "\n",
    "model_simple_features = LogisticRegression(\n",
    "    penalty='none', solver='lbfgs'\n",
    ")\n",
    "\n",
    "model_simple_features.fit(X_test, y_test)\n",
    "\n",
    "probs = model_simple_features.predict_proba(X_test)[:, 1]\n",
    "y_hat = (probs > 0.5).astype(np.int64)\n",
    "\n",
    "draw_results(\n",
    "    X_test[:, 0], X_test[:, 1], color=probs, \n",
    "    plot_title=\"Logistic regression predicted probs (no feature eng)\"\n",
    ")\n",
    "draw_results(\n",
    "    X_test[:, 0], X_test[:, 1], color=y_hat, \n",
    "    plot_title=\"Logistic regression prediction (no feature eng)\"\n",
    ")\n",
    "\n",
    "accuracy = np.mean(y_test == y_hat)\n",
    "print(f\"Accuracy on training set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the labels classified by the model with the true labels, we notice that the simple logistic regression model is not ideal partly because the true decision boundary is nonlinear. In order to improve on the model, we will engineer new features. \n",
    "\n",
    "With the checkerboard dataset, we engineered a new feature, $x_1 \\times x_2$, which was just what we needed. For this dataset, the feature that we need is a little more complicated.\n",
    "\n",
    "Instead, we'll take inspiration from neural networks, and add many random features, where each is a random linear combination of the inputs, where the coefficients will be random numbers between -1 and 1.\n",
    "\n",
    "Don't forget that we also need to apply a nonlinearity, or else the linear combinations won't help us when applying logistic regression. In this example, we'll use the sigmoid function. For example, one feature might be $\\sigma(-0.37x_1 + 0.82x_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Add random features\n",
    "\n",
    "Complete the cell below to add random features to the dataset. As described above, we first generate a pair of coefficients $(c_1, c_2)$ uniformly random from $(-1, 1)$ and then for botht the training set and the test set, add an additional column whose values are $c_1x_1 + c_2 x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def add_random_feature(train_data, test_data):\n",
    "    # Returns the modified train_data and test_data\n",
    "    coeffs = ... # TODO: fill in\n",
    "    # This code gives the feature a convenient name\n",
    "    feat_name = f\"Ïƒ({coeffs[0]:0.2f}x1 + {coeffs[1]:0.2f}x2)\"\n",
    "\n",
    "    for dataset in (train_data, test_data):\n",
    "        linear_combination = np.dot(dataset[['x1', 'x2']], coeffs)\n",
    "    \n",
    "        feature = ... # TODO: fill in\n",
    "        dataset[feat_name] = feature\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "ring_train_copy3a, ring_test_copy3a = ring_train.copy(), ring_test.copy()\n",
    "for i in range(23):\n",
    "    add_random_feature(ring_train_copy3a, ring_test_copy3a)\n",
    "\n",
    "assert((ring_train_copy3a.iloc[:, 4:].values < 1).all())\n",
    "assert((ring_train_copy3a.iloc[:, 4:].values > 0 ).all())\n",
    "assert((ring_test_copy3a.iloc[:, 4:].values < 1 ).all())\n",
    "assert((ring_test_copy3a.iloc[:, 4:].values > 0 ).all())\n",
    "assert(ring_train_copy3a.shape == (500, 26))\n",
    "assert(ring_test_copy3a.shape == (750, 26))\n",
    "\n",
    "\n",
    "print(\"tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code you completed in 3a, we can now add 10 random features to both the training set and the test set using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell uses the code you wrote to add 10 random features to the\n",
    "# train and test sets.\n",
    "\n",
    "ring_train_feats = ring_train.copy()\n",
    "ring_test_feats = ring_test.copy()\n",
    "for i in range(10):\n",
    "    ring_train_feats, ring_test_feats = (\n",
    "        add_random_feature(ring_train_feats, ring_test_feats)\n",
    "    )\n",
    "ring_train_feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a new logistic regression model with these 10 additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to write any code here: just understand.\n",
    "X_train = ring_train_feats.iloc[:, 1:].values\n",
    "y_train = ring_train_feats['y'].values\n",
    "\n",
    "X_test = ring_test_feats.iloc[:, 1:].values\n",
    "y_test = ring_test_feats['y'].values\n",
    "\n",
    "model_features = LogisticRegression(\n",
    "    penalty='none', solver='lbfgs'\n",
    ")\n",
    "\n",
    "model_features.fit(X_train, y_train)\n",
    "\n",
    "probs = model_features.predict_proba(X_test)[:, 1]\n",
    "y_hat = (probs > 0.5).astype(np.int64)\n",
    "\n",
    "draw_results(\n",
    "    X_test[:, 0], X_test[:, 1], color=probs, \n",
    "    plot_title=\"Logistic regression predicted probs (random features)\"\n",
    ")\n",
    "\n",
    "draw_results(\n",
    "    X_test[:, 0], X_test[:, 1], color=y_hat, \n",
    "    plot_title=\"Logistic regression prediction (random features)\"\n",
    ")\n",
    "\n",
    "accuracy = np.mean(y_test == y_hat)\n",
    "print(f\"Accuracy on training set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the accuracy is already better. Now that our model has improved, let's try to interpret it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Coefficients\n",
    "\n",
    "Fill in the blanks in the following code that creates a dataframe with the coefficients from the logistic regression model.\n",
    "\n",
    "*Hint: you may find it helpful to refer to the demo from lecture.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below\n",
    "feature_names = ring_train_feats.columns[1:]\n",
    "logistic_coeff_vals = ... # TODO fill in\n",
    "len(logistic_coeff_vals)\n",
    "coefficient_df = pd.DataFrame(\n",
    "    {'feature': feature_names, 'coefficients': logistic_coeff_vals}\n",
    ")\n",
    "\n",
    "coefficient_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "assert(coefficient_df.shape == (12, 2))\n",
    "assert(coefficient_df['feature'][0] == 'x1')\n",
    "assert(coefficient_df['feature'][1] == 'x2')\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c Interpretibility\n",
    "\n",
    "This model has better performance on the data but at the same time, it loses some interpretability. Explain why this logistic regression model is harder to interpret than the simpler (and worse-performing) one from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on completing the lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('cute_flemish.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.axes.get_xaxis().set_visible(False)\n",
    "imgplot.axes.get_yaxis().set_visible(False)\n",
    "print(\"Yay, you've made it to the end of Lab 9!\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
