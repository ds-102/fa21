{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2244224",
   "metadata": {},
   "source": [
    "# Lab 5: Linear Models, Confidence Intervals and Credible Intervals, Bootstrap and Hypothesis Testing\n",
    "Welcome to the Data 102 Fall 201 Lab 5. In this lab, we are going to look at a variety of topics including Linear Models, Confidence Intervals and Credible Intervals, Bootstrap and Hypothesis Testing. The lab assignment may seem long,. but there are only a few simple lines of code to fill in in this lab assignment. There are a lot of texts in this assignment. Please make sure you carefully read through them. \n",
    "\n",
    "#### The code and responses you need to write are commented out with a message  `TODO: fill in`. There is additional documentation for each part as you go along.\n",
    "\n",
    "##### Please read carefully the introduction and the instructions to each problem.\n",
    "\n",
    "## Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c1eba8",
   "metadata": {},
   "source": [
    "Collaborators:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55e8d9",
   "metadata": {},
   "source": [
    "## Gradescope Submission\n",
    "To submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Wednesday, October 13, 2021 at 11:59 PM. PST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, interactive\n",
    "import itertools\n",
    "import hashlib\n",
    "from scipy.stats import poisson, norm, gamma\n",
    "#!pip install pymc3\n",
    "import statsmodels.api as sm\n",
    "  \n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "try:\n",
    "    from pymc3 import *\n",
    "    import pymc3 as pm\n",
    "except:\n",
    "    ! pip install pymc3\n",
    "    from pymc3 import *\n",
    "    import pymc3 as pm\n",
    "\n",
    "import arviz as az\n",
    "\n",
    "def get_hash(num, significance = 4):\n",
    "    num = round(num, significance)\n",
    "    \"\"\"Helper function for assessing correctness\"\"\"\n",
    "    return hashlib.md5(str(num).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bfa91b",
   "metadata": {},
   "source": [
    "# Part I: GLM\n",
    "# Question 1: Poisson Regression\n",
    "\n",
    "# Atlantic Hurricane Season\n",
    "\n",
    "With 30 named storms, the 2020 Atlantic hurricane season was the most active on record. Climate scientists argue that the culprit is human induced global warming. There is a an evergrowing body of research linking increased average temperatures and rising sea levels to more frequent, more intense and more destructive storms. \n",
    "\n",
    "In this lab we will investigate the number of named storms recorded since 1880, and we will argue that there is a statistically significant relationship between rising Sea Surface Temperature (SST) and the frequency of named storms.\n",
    "\n",
    "For this lab we extracted the number of tropical storms from the [HURDAT Database](https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2019-052520.txt). We also extracted data on Sea Surface Temperatures from the [National Center for Atmosferic Research](https://climatedataguide.ucar.edu/climate-data/global-surface-temperature-data-gistemp-nasa-goddard-institute-space-studies-giss). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce1f00",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Just run the code to load the data\n",
    "data_source = \"hurricane_data.csv\"\n",
    "df = pd.read_csv(data_source)\n",
    "df = df[[\"Year\", \"Num_Storms\", \"Temp_Anomaly\"]]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f963f37",
   "metadata": {},
   "source": [
    "The `Num_Storms` column contains the number of named storms recorded each year between 1880 and 2019. The `Temp_Anomaly` column contains the deviation in yearly SST from the mean of 1951-1980."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df982f03",
   "metadata": {},
   "source": [
    "In this question we will model the number of named storms in Year $i$ as: \n",
    "$C_i \\sim Poisson(\\lambda_i)$, where $\\lambda_i = exp(q_0 + q_1 X_i)$, and $X_i$ is the SST deviation in Year $i$.\n",
    "\n",
    "This isn't something that we can easily solve from scratch, so we have to use software packages. For Frequentist Poisson Regression we will use [`statsmodels.api`](https://www.statsmodels.org/stable/glm.html) and for the Bayesian counterpart we will use [`PYMC3`]().\n",
    "\n",
    "## 1.a Frequentist Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ed272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Poisson GLM model where Temp_Anomaly is a covariate (exogenous variable): No need to modify\n",
    "freq_model = sm.GLM(df.Num_Storms, exog = sm.add_constant(df.Temp_Anomaly), \n",
    "                  family=sm.families.Poisson())\n",
    "freq_res = freq_model.fit()\n",
    "print(freq_res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d17cad",
   "metadata": {},
   "source": [
    "### TODO: Inspecting the results of fitting `freq_model`. Does the model suggest that increased SST relate to more storms? Is the influnce of SST on number of storms statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0ed03",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517f825",
   "metadata": {},
   "source": [
    "## 1.b Bayesian Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e845825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify\n",
    "with pm.Model() as bayes_model:\n",
    "    glm.GLM.from_formula('Num_Storms ~ Temp_Anomaly', df, family=glm.families.Poisson()) \n",
    "    # PYMC3 automatically uses exponential link function and adds an intercept term\n",
    "    trace_poisson = pm.sample(1000, cores=2, target_accept=0.95, init='adapt_diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior distribution for the intercept and Temp_Anomaly coefficients\n",
    "az.plot_posterior(trace_poisson, ['Intercept', 'Temp_Anomaly'], round_to = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc264c8",
   "metadata": {},
   "source": [
    "**Note: \"HDI\" in the plots above stands for Highest Density Interval, which is another term for credibal interval.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538931b2",
   "metadata": {},
   "source": [
    "### TODO: Compare the results of `freq_model` in 1.a with the plot above. Are the estimates of Frequentist and Bayesian Regression close to each-other? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a8962",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d148d5",
   "metadata": {},
   "source": [
    "## 1.c Posterior Predictive Checks\n",
    "\n",
    "In order to validate our Bayesian model we perform Posterior Predictive Checks (PPC). After performing Bayesian Regression we have access to a generating distribution for counts $C'_i|X_i$. The crux of PPC is to sample such counts and to compare them to the original historical data.\n",
    "\n",
    "The code below computes PPC samples and plots their distribution. Note that here, $C$ has been renamed $y$, and the band labeled \"Posterior predictive $y$\" is a collection of curves, each of which is the density plot of $y$ for a given draw of the coefficient vector $\\beta$ from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7334470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify\n",
    "with pm.Model() as poisson_model:\n",
    "    glm.GLM.from_formula('Num_Storms ~ Temp_Anomaly', df, family=glm.families.Poisson())\n",
    "    # PYMC3 automatically uses exponential link function and adds an intercept term\n",
    "    trace_poisson = pm.sample(1000, cores=2, target_accept=0.95, init='adapt_diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ce898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "# Sample C'_i|X_i \n",
    "with poisson_model:\n",
    "    poisson_ppc = pm.sample_posterior_predictive(trace_poisson)\n",
    "    poisson_ppc['y'] = poisson_ppc['y'] + 0.0\n",
    "    ppc_poisson = az.from_pymc3(trace_poisson, posterior_predictive=poisson_ppc)\n",
    "        \n",
    "# Plot PPC samples\n",
    "\n",
    "az.plot_ppc(ppc_poisson)\n",
    "plt.xlabel('y = Number of named storms')\n",
    "plt.title('Bayesian Poisson Regression with SST covariate')\n",
    "plt.axis([-2, 45, -0.01, 0.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3edf8",
   "metadata": {},
   "source": [
    "We can also estimate by employing a Bayesian perspective without using the covariate $X_i's$.\n",
    "\n",
    "Following this perspective, the model is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613aec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify\n",
    "with pm.Model() as simple_poisson_model:\n",
    "    glm.GLM.from_formula('Num_Storms ~ 1', df, family=glm.families.Poisson())\n",
    "    # PYMC3 automatically uses exponential link function and adds an intercept term\n",
    "    trace_simple_poisson = pm.sample(1000, cores=2, target_accept=0.95, init='adapt_diag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7898b5",
   "metadata": {},
   "source": [
    "Similarly perform Predictive Posterior Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "# Sample C'_i|X_i \n",
    "with simple_poisson_model:\n",
    "    simple_poisson_ppc = pm.sample_posterior_predictive(trace_simple_poisson)\n",
    "    simple_poisson_ppc['y'] = simple_poisson_ppc['y'] + 0.0\n",
    "    ppc_simple_poisson = az.from_pymc3(trace_simple_poisson, posterior_predictive=simple_poisson_ppc)\n",
    "        \n",
    "# Plot PPC samples\n",
    "#ppc_simple_poisson = az.from_pymc3(trace_simple_poisson, posterior_predictive=simple_poisson_ppc)\n",
    "az.plot_ppc(ppc_simple_poisson)\n",
    "plt.xlabel('C = Number of named storms')\n",
    "plt.title('Bayesian Poisson WITHOUT SST covariante')\n",
    "plt.axis([-2, 45, -0.01, 0.2])\n",
    "print(plt.axis())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c10c74",
   "metadata": {},
   "source": [
    "### TODO: Compare the two plots above. In your opinion, which model is a better fit for the observed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c16019",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f5d21",
   "metadata": {},
   "source": [
    "# Part II: Bootstrap and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582cb49",
   "metadata": {},
   "source": [
    "In the following questions, we are going to experiment with bootstrap and hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2997a79",
   "metadata": {},
   "source": [
    "## Goal: testing for multimodality\n",
    "Suppose that $X_1, . . . , X_n$ are i.i.d. samples from a distribution with continuous density $p(x)$.\n",
    "One important property of the density $p(x)$ is the number of modes it has. Multimodality of\n",
    "the density indicates a heterogeneity in the data. In this lab, we will demonstrate how to perform a hypothesis test to determine whether a distribution is multimodal. We'll use the Bootstrap Method to perform this hypothesis test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9604ef",
   "metadata": {},
   "source": [
    "## Galaxy data\n",
    "\n",
    "We will be working with galaxy data. The dataset contains velocities in km/sec of 82 galaxies from 6 well-separated conic sections of an unfilled survey\n",
    "of the Corona Borealis region. The distribution\n",
    "of galaxy velocities provides information about the structure of the far universeâ€”in\n",
    "particular, a multimodal distribution of velocities is seen as evidence for the existence\n",
    "of voids and superclusters (links to wikipedia pages on [voids](https://en.wikipedia.org/wiki/Void_(astronomy)) and [superclusters](https://en.wikipedia.org/wiki/Supercluster)).\n",
    "\n",
    "Let $X_1, . . . , X_{n}$ be the velocities of each galaxy, where $X_i$ is the velocity of the $i$th galaxy and we observe $n=82$ galaxies.\n",
    "\n",
    "We want to test whether or not the distribution that the $X_i$'s are drawn from is multimodal. Let the null and alternative hypotheses be defined as follows:\n",
    "\n",
    "$$H_0: m(p) = 1$$ \n",
    "$$H_A: m(p) > 1$$ \n",
    "\n",
    "where $p$ is the distribution of galaxy velocities, and $m(p)$ is the number of modes of a distribution $p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901bbf51",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "First, we'll load the data and see what the histogram looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca7bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies_df = pd.read_csv('galaxies.csv', index_col=0, header=0, names=['velocity'])\n",
    "# Divide all entries by 1000 for ease of reading.\n",
    "galaxies_df['velocity'] = galaxies_df['velocity'] / 1000\n",
    "X_observed = np.array(galaxies_df['velocity'])\n",
    "galaxies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95698b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_observed, bins=15)\n",
    "plt.title(\"Histogram of galaxy velocities\")\n",
    "plt.xlabel(\"Velocity of galaxy, X (thousands of km/s)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c010f",
   "metadata": {},
   "source": [
    "# Question 2. Estimating the density and test statistic\n",
    "\n",
    "In order to infer whether or not the $X_1,...,X_{n}$ were drawn from a multimodal distribution, we need to come up with a test statistic that reflects how suitable a unimodal distribution is for\n",
    "modeling this data. \n",
    "\n",
    "To do this, we first need to come up with a model for the density function itself. In this lab you will have the chance to learn about a non-parametric density estimatation technique called *kernel density estimation*. This was also covered briefly in Data 100. To revisit the concept, see [data 100 textbook](https://www.textbook.ds100.org/ch/11/viz_principles_2.html#principles-of-smoothing).\n",
    "\n",
    "#### Kernel Density Estimation\n",
    "Given a set of points $X_1, X_2, \\ldots, X_n \\sim p(x)$. The goal of Kernel Density Estimation is to estimate the density $p(x)$ via a function $\\hat{p}_h(x)$ of the form:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{p}_h(x) = \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - X_i}{h}\\right) \n",
    "\\end{align}\n",
    " \n",
    "\n",
    "The function $K$ is a non-increasing function that takes only non-negative values. These functions are known as kernel functions. They are often used to capture the influence of each data point $X_i$ on the density of an arbitrary point $x$. A common choice of kernel is the Gaussian kernel, which is what we will use in this lab: \n",
    "\n",
    "$$K(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0933da",
   "metadata": {},
   "source": [
    "In addition, the parameter $h > 0$ is a bandwidth parameter that captures how close data points $X_i$ must be to $x$ to influence its density: for larger values of $h$, more data points have an\n",
    "influence on the density at $x$, whereas for smaller values of $h$, only data points very close\n",
    "to $x$ influence it.\n",
    "\n",
    "It can be shown that the number of modes of $\\hat{p}_h(x)$ (a.k.a. $m(\\hat{p}_h(x))$) decreases monotonically as $h$ increases. Therefore, $h$ will be an important tool in our hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c17f4",
   "metadata": {},
   "source": [
    "## 2.a. Plot the density estimates $\\hat{p}_h(x)$\n",
    "\n",
    "Using the kernel function $K(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$, we will first plot $\\hat{p}_h(x)$ to get a sense of what these density estimates look like for different values of $h$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{p}_h(x) &= \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - X_i}{h}\\right) \\\\\n",
    "&= \\frac{1}{nh} \\sum_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - X_i)^2}{2h^2}\\right) \\\\\n",
    "&= \\frac{1}{nh \\sqrt{2\\pi}} \\sum_{i=1}^n \\exp\\left(-\\frac{(x - X_i)^2}{2h^2}\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Using the final simplified form above, implement a function that calculates $\\hat{p}_h(x)$ at a given point $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: fill in.\n",
    "from math import pi, exp, sqrt\n",
    "def p_hat(x, h, X):\n",
    "    \"\"\"Calculates p_hat(x) at a single point x, where the bandwidth of the kernel function is h.\n",
    "    \n",
    "    Inputs: \n",
    "      x : float, point at which to evaluate the function p_hat.\n",
    "      h : float, bandwidth parameter in kernel function.\n",
    "      X : array of floats of length n containing the observed galaxy velocities.\n",
    "      \n",
    "    Outputs:\n",
    "      y: float, the value of p_hat(x) at the given point x.\n",
    "    \"\"\"\n",
    "    y = ... # TODO: fill in \n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bed160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests: Do not modify\n",
    "x_values = [10, 15, 20, 25, 30, 35]\n",
    "h_values = [0.5, 1, 2, 5, 10]\n",
    "inputs = list(itertools.product(x_values, h_values))\n",
    "outputs = [p_hat(*inp, X_observed) for inp in inputs]\n",
    "hash_list = ['bfde2cf30f9e9ce79408d99cab3e8bc8', '0574a27738923dd052ed0b873c176afc', '78b6d148d48bbea1345c899ca9e46909', \n",
    " 'f4ee6417f4b3ac1be9eb6568e73144b6', '3ffdc10d8a61ad159f223313dfbc03e2', '605849c8f83de4da7c6fd144d7f58826', \n",
    " '3f372d09d37f32da442cb9ee0ac460f0', '5b3bc4848902c9a21de80767f9e339b8', '71c61135ff01aa84ee86864b86711f5b', \n",
    " '04d8f0e70a01305f62dbadaa76a8f7cb', '45bd1e472af7b3bc57dc9312833c17a0', '45367152f5d7346868703d4980f60e03', \n",
    " '27e795eb0f314edf0479737480ab0f2a', 'd16aea76e1e1217b7eb5f7395948d364', 'd9cd4af75fe14d77a6faa0335f83b8c0', \n",
    " '2f6f54bd4207339ea29d730563c34b14', '2e0bfa827d51449a197fae6ab9f4804c', 'e0e4a546725d40f259b552ae41932f38', \n",
    " 'ee15c394e25320ba6ce793eeaef72cdb', '9740966b3dc9d6fd0efba0313c963536', '30565a8911a6bb487e3745c0ea3c8224', \n",
    " '7f77a1b9954bf3ae6f43c6298871aeda', 'e5b43ca16f1f9cf6cbcfa71cdc7220ae', '83441ddfbbdcf553b5efe63b81eb1832', \n",
    " '2a6b856fad8c51aed13cfd58c948b380', '3b15219be5ebd7c0831dc46746a52d3c', '870b8425c55942fe40463327364cb546', \n",
    " 'a2eb92865432da0c58cef552428f4ed1', 'dfff58ed1f9b7657068b28b69b62d70e', 'ee217454afa3c02cb3f1799b5aad3608']\n",
    "for i, inp in enumerate(inputs):\n",
    "    assert(get_hash(outputs[i])==hash_list[i])\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Just run this cell after you pass the validation tests above\n",
    "def plot_density_estimate(h):\n",
    "    x_values = np.arange(0, 45, 0.5)\n",
    "    y_values = [p_hat(x, h, X_observed) for x in x_values]\n",
    "    fig = plt.figure(figsize=(9,6))\n",
    "    plt.hist(X_observed, bins=15, density=True, label=\"Histogram of observed values\")\n",
    "    plt.plot(x_values, y_values, label = \"Estimated kernel density\")\n",
    "    plt.title(\"Density $\\hat{p}_h(x)$\")\n",
    "    plt.ylabel(\"Density $\\hat{p}_h(x)$\")\n",
    "    plt.xlabel(\"Velocity, x\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interactive plot: Do not modify\n",
    "interactive_plot = interactive(plot_density_estimate, h=(0.1, 4, 0.1))\n",
    "interactive_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c984918",
   "metadata": {},
   "source": [
    "## 2.b. Questions:\n",
    "### (i) Start with a small value of $h=0.1$, then slide the value of $h$, what do you observe? \n",
    "### (ii) Does the density estimate $\\hat{p}_h(x)$ seem to contain more modes for higher values of $h$ or lower values of $h$?\n",
    "### (iii) For what values of $h$ (small or large), does $\\hat{p}_h(x)$ fit the current data more closely? Would this value generalize well to other unseen data?\n",
    "\n",
    "TODO: fill in your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba205df4",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407bd50",
   "metadata": {},
   "source": [
    "# Question 3. Count the modes of $\\hat{p}_h(x)$\n",
    "\n",
    "### There are no todo's in this question except for a simple fill-in-the-blank question at the end. Make sure you read through the questions and understand the solutions before you move on.\n",
    "\n",
    "Now we will write a function that counts the number of modes of a given density estimate $\\hat{p}_h(x)$. This is the $m(p)$ function mentioned above. \n",
    "\n",
    "To do this, we say that a density function $p$ has a mode everywhere the function $p(x)$ has an increase followed by a decrease. That is, $p(x)$ has an additional mode for each time the derivative of the function $p(x)$ transitions from non-negative to negative.\n",
    "\n",
    "Following the above definition, to count the number of modes in $\\hat{p}_h(x)$, first we will take the derivative, $$\\frac{d}{dx}\\hat{p}_h(x).$$\n",
    "\n",
    "Then, we will count the number of times that the derivative transitions from positive (or 0) to negative over a grid of $x$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4a5db",
   "metadata": {},
   "source": [
    "## 3.a. Calculate the derivative $\\frac{d}{dx}\\hat{p}_h(x).$\n",
    "\n",
    "Using the kernel function $K(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$, we will now calculate the derivative $\\frac{d}{dx}\\hat{p}_h(x)$ by applying the chain rule.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx}\\hat{p}_h(x) &= \\frac{d}{dx} \\frac{1}{nh} \\sum_{i=1}^n K\\left(\\frac{x - X_i}{h}\\right) \\\\\n",
    "&= \\frac{1}{nh} \\sum_{i=1}^n \\frac{d}{dx} K\\left(\\frac{x - X_i}{h}\\right) \\\\\n",
    "&= \\frac{1}{nh} \\sum_{i=1}^n \\frac{1}{h} K'\\left(\\frac{x - X_i}{h}\\right) \\\\\n",
    "&= \\frac{1}{nh^2} \\sum_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} \\frac{-(x - X_i)}{h} \\exp\n",
    "\\left(-\\frac{\\left(\\frac{x - X_i}{h}\\right)^2}{2}\\right) \\\\\n",
    "&= \\frac{1}{nh^3 \\sqrt{2\\pi}} \\sum_{i=1}^n (X_i - x)\\exp\n",
    "\\left(-\\frac{(x - X_i)^2}{2h^2}\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "### Using the final simplified form of the derivative above, implement a function that calculates $\\frac{d}{dx}\\hat{p}_h(x)$ at a given point $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hat_derivative(x, h, X):\n",
    "    \"\"\"Calculates the derivative d/dx p_hat(x) at a single point x.\n",
    "    \n",
    "    Inputs: \n",
    "      x : float, point at which to evaluate the derivative.\n",
    "      h : float, bandwidth parameter in p_hat.\n",
    "      X : array of floats of length n containing the observed galaxy velocities.\n",
    "      \n",
    "    Outputs:\n",
    "      y_prime : float, the derivative d/dx phat_h(x) at the given point x.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    total = np.sum((X - x) * np.exp(-((x - X)**2) / (2 * h**2)))\n",
    "    y_prime = total / (n * h**3 * sqrt(2 * pi))\n",
    "    return(y_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e4dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify: Just run this cell after you pass the validation tests above\n",
    "def plot_density_derivative(h):\n",
    "    x_values = np.arange(4, 36, 0.5)\n",
    "    y_values = [p_hat_derivative(x, h, X_observed) for x in x_values]\n",
    "    fig = plt.figure(figsize=(9,6))\n",
    "    plt.plot(x_values, y_values)\n",
    "    plt.axhline(0, c = 'k', ls = \"--\")\n",
    "    plt.title(\"Derivative of the density $\\hat{p}_h(x)$\")\n",
    "    plt.xlabel(\"Velocity, x\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interactive plot: Do not modify\n",
    "interactive_plot = interactive(plot_density_derivative, h=(0.1, 4, 0.1))\n",
    "interactive_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0cafb6",
   "metadata": {},
   "source": [
    "We notice that when $h$ is really small the derivative is very 'wiggly' and it frequently crosses the 0 line. As $h$ increases, it crosses the 0 line less frequently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55bd50",
   "metadata": {},
   "source": [
    "## 3.b. Count the number of modes in $\\hat{p}_h(x)$\n",
    "\n",
    "Using the derivative calculated above, we will now count the number of modes in $\\hat{p}_h(x)$.\n",
    "\n",
    "To do this, we will evaluate the derivative $\\frac{d}{dx}\\hat{p}_h(x)$ at a grid of points $x_1,...,x_m$ evenly spaced between $5$ and $35$ (the lower and upper bounds on the velocities in the data), and count the number of times that the derivative crosses from positive to negative.  The use of a grid of $x$'s isn't a perfect measurement of the mode count, since if we don't evaluate the derivative at enough points that are close enough together, we may miss some modes. In this lab, we will make sure that the grid we use is fine enough to accurately count the number of modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the modes of phat using the derivative implemented above.\n",
    "def count_modes(x_values, h, X):  \n",
    "    \"\"\"Counts the number of modes in p_hat(x), approximated over the given grid of x_valies.\n",
    "    \n",
    "    Counts a mode every time the derivative of p_hat(x) crosses from positive (or 0)\n",
    "    to negative over the given grid of x_values.\n",
    "    \n",
    "    Inputs: \n",
    "      x_values : array of floats of length m \n",
    "      containing points at which to evaluate the derivative.\n",
    "      h: float, bandwidth parameter in phat_h.\n",
    "      X: array of floats of length n containing the observed galaxy velocities.\n",
    "      \n",
    "    Outputs:\n",
    "      num_modes : int, the number of modes in p_hat(x).\n",
    "    \"\"\"\n",
    "    # First calculate the derivative at all points in x_values.\n",
    "    all_derivatives = [p_hat_derivative(x, h, X) for x in x_values]\n",
    "    \n",
    "    # Iterate through all of the calculated derivatives, \n",
    "    # and add a mode every time the derivative crosses from positive (or 0) to negative.\n",
    "    num_modes = 0\n",
    "    for i in range(0, len(all_derivatives)-1):\n",
    "        if (all_derivatives[i] >= 0) and (all_derivatives[i + 1] < 0):\n",
    "            num_modes += 1\n",
    "    \n",
    "    return num_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc17db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No need to modify: Plot the number of modes for different values of h.\n",
    "# This cell may take a few seconds to run.\n",
    "x_values = np.arange(5,35,0.05)\n",
    "h_values = np.arange(0.3,4,0.1)\n",
    "mode_counts = [count_modes(x_values, h, X_observed) for h in h_values]\n",
    "\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "plt.plot(h_values, mode_counts)\n",
    "plt.title(\"Number of modes in $\\hat{p}_h(x)$\")\n",
    "plt.ylabel(\"Number of modes\")\n",
    "plt.xlabel(\"Bandwidth h\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84da65",
   "metadata": {},
   "source": [
    "## 3.c. (To-do) Fill-in-the-blanks\n",
    "Based on your observation above, we can develop a stategy to find the number of modes. Fill in the blanks below.\n",
    "\n",
    "\"*To find the number of modes, we evaluate the (**Blank 1**)  on a grid of points, and count the number of times it goes from (**Blank 2**) to (**Blank 3**).\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f30c44",
   "metadata": {},
   "source": [
    "**Your answer here (one word for each blank):**\n",
    "\n",
    "Blank 1: \n",
    "\n",
    "Blank 2: \n",
    "\n",
    "Blank 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d24f4d",
   "metadata": {},
   "source": [
    "# Question 4. Hypothesis test\n",
    "\n",
    "Now that we've defined the density estimate $\\hat{p}_h(x)$ and figured out how to count the number of modes in $\\hat{p}_h(x)$, we will move on to testing whether or not a multimodal distribution can reasonably fit our data $X_1,...,X_n$.\n",
    "\n",
    "In the plot in part 2.b. you should have observed that the number of modes in $\\hat{p}_h(x)$ decreases monotonically as $h$ increases. Let $H_1$ be the minimal bandwidth value $h$ for which $\\hat{p}_h(x)$ is unimodal. \n",
    "\n",
    "\\begin{align}\n",
    "    H_1 & = \\min \\{h \\colon m(\\hat{p}_h) = 1, \\, m(\\hat{p}_{h'}) > 1 \\text{ for all } h' < h\\}.\n",
    "\\end{align}\n",
    "\n",
    "Similarly we define $H_k$ to be the minimal bandwidth value $h$ for which $\\hat{p}_h(k)$ has $k$ modes:\n",
    "\n",
    "\\begin{align}\n",
    "    H_k & = \\min \\{h \\colon m(\\hat{p}_h) = k, \\, m(\\hat{p}_{h'}) > k \\text{ for all } h' < h\\}.\n",
    "\\end{align}\n",
    "\n",
    "We will use $H_k$ as the test statistic. \n",
    "\n",
    "Notice that $H_k$ depends on the data $X$, because the function $\\hat{p}_h(x)$ depends on the data $X$.\n",
    "\n",
    "For our particular observed dataset $X_{observed}$, let $h_k$ be the observed minimal bandwidth value $h$ for which $\\hat{p}_h(x)$ has $k$ modes.\n",
    "\n",
    "\n",
    "## Calculate $H_k$\n",
    "The first thing we need to do is calculate $H_k$ for a given dataset $X$. To do this, we will try different values of $h$ until we find the smallest value such that the density estimate $\\hat{p}_h$ has $k$ modes. The function below accomplishes that. Take a few minutes to examine it and understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eee0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just understand what this function is doing.\n",
    "def find_hk(x_values, X, h_min=0.3, h_max=4, h_err = 0.01, k=1):\n",
    "    \"\"\"\n",
    "    Calculates h_k, the minimum bandwidth h such that the density estimate p_hat has k modes.\n",
    "    Chooses h_k from within an interval bounded by h_min and h_max, within error h_err.\n",
    "    \n",
    "    Inputs:\n",
    "      x_values: array of floats containing points x to use to count the number of modes in p_hat.\n",
    "      X: array of floats of length n containing the observed galaxy velocities.\n",
    "      h_min: float, minimum h to try.\n",
    "      h_max: float, maximum h to try.\n",
    "      h_err: float, allowed error of h, or step size of hs to try between h_min and h_max.\n",
    "      k: number of modes being tested in the hypothesis test.\n",
    "      \n",
    "    Returns:\n",
    "      h_k: minimum bandwith h among candidate h_values such that p_hat has k modes.\n",
    "    \"\"\"\n",
    "    # Perform a binary search to find the minimum bandwith hk.\n",
    "    h_opt = 0\n",
    "    modes_min = count_modes(x_values, h_min, X)\n",
    "    modes_max = count_modes(x_values, h_max, X)\n",
    "    while h_max - h_min > h_err:\n",
    "        h_opt = (h_min + h_max) / 2\n",
    "        modes_opt = count_modes(x_values, h_opt, X)\n",
    "        if modes_opt > k:\n",
    "            h_min = h_opt\n",
    "            modes_min = modes_opt\n",
    "        else:\n",
    "            h_max = h_opt\n",
    "            modes_max = modes_opt\n",
    "    return h_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e689a59",
   "metadata": {},
   "source": [
    "The function above calculates the test statistic $H_k$. To calculate the value $h_1$ for the null hypothesis, we apply this same function over the observed data $X_1,...,X_n$. Run the cell below and compare the outputs with the plot in 2.b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell to calculate the value of h_k using the function above.\n",
    "# This cell might take a minute or so to run\n",
    "x_values = np.arange(5,35,0.05)\n",
    "for k in range(1,10):\n",
    "    hk = find_hk(x_values, X_observed, k=k)\n",
    "    print(\"For k = {}. Estimate value of h_{}: {:.4f}\".format(k, k, hk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad446ead",
   "metadata": {},
   "source": [
    "##  Computing the $p$-value\n",
    "\n",
    "Let's say we are trying to test if the distribution of galaxies' velocities is unimodal. The corresponding test statistic is $H_1$ and the observed realization is $h_1$. Therefore, the $p$-value for our hypothesis test is: \n",
    "\n",
    "$$P_{0}(H_1 \\geq h_1)$$\n",
    "\n",
    "where $P_0$ is the probability under the null hypothesis that the $X_i$ are drawn from a unimodal distribution. This $p$-value represents the probability under the null hypothesis that we observe a value as extreme as $h_1$ for the minimum width parameter.\n",
    "\n",
    "To perform a hypothesis test at significance level $\\alpha$, we reject the null hypothesis if the $p$-value is less than $\\alpha$:\n",
    "$$P_{0}(H_1 \\geq h_1) \\leq \\alpha. $$\n",
    "\n",
    "Now, we need to calculate the $p$-value. Unfortunately, we don't have a closed form for the distribution of the test statistic $H_1$ under the null hypothesis that the $X_i$ are drawn from a unimodal distribution. In fact, we don't even know what distribution the $X_i$ are drawn from, only that it's unimodal (under the null hypothesis)! Still, to estimate the distribution of the test statistic $H_1$, we need to pick some distribution to use for the distribution of the $X_i$'s under the null hypothesis.\n",
    "\n",
    "Among the parameterized densities $\\hat{p}_h(x)$, the density $\\hat{p}_{h_1}(x)$ is the closest unimodal distribution  to the empirical distribution $p$ of the observed data. So, we will use $\\hat{p}_{h_1}(x)$ as the distribution of the $X_i$'s under the null hypothesis.\n",
    "\n",
    "Therefore, the $p$-value that we will calculate is $$P_{X_i \\sim \\hat{p}_{h_1}}(H_1 \\geq h_1).$$\n",
    "\n",
    "More generally, if instead we want to test if the distribution of galaxies' velocities has at most $k$ modes can be calculated as:\n",
    "$$P_{0}(H_k \\geq h_k) \\approx P_{X_i \\sim \\hat{p}_{h_k}}(H_k \\geq h_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040d948",
   "metadata": {},
   "source": [
    "##  4.a. Sampling from $\\hat{p}_{h_k}$ using the Bootstrap Method\n",
    "\n",
    "To calculate the $p$-value, we will first draw i.i.d. samples from $\\hat{p}_{h_k}$, and then observe the number of times that the $H_k$ calculated from those samples is greater than or equal to $h_k$. We will use the bootstrap to draw the i.i.d. samples from $\\hat{p}_{h_k}$.\n",
    "\n",
    "Let $Z^{*} = (Z_1^*, \\ldots, Z_{82}^*)$ denote a bootstrap sample from the dataset $X_{observed}$. It can be shown that by adding some scaled noise to the bootstrap samples we can obtain samples from the null distribution. More precisely: \n",
    "$Z_i^* + h_k \\epsilon_i$ for $\\epsilon_i \\sim \\mathcal{N}(0, 1)$ gives i.i.d. samples from $\\hat{p}_{h_k}$.\n",
    "\n",
    "This leads to the following bootstrap algorithm: \n",
    "\n",
    "1. Draw $B$ independent bootstrap samples $Z^{*(1)}, \\ldots, Z^{*(B)}$ from the observed data. Then add some noise to get samples  $X^{*(1)}, \\ldots, X^{*(B)}$ from the null distribution $\\hat{p}_{h_1}$:\n",
    "    \\begin{align}\n",
    "        X_i^{*(b)} & = Z_i^{*(b)} + h_k \\epsilon_i^{(b)} \\\\\n",
    "        \\epsilon_i^{(b)} & \\sim \\mathcal{N}(0, 1)\n",
    "    \\end{align}\n",
    "    \n",
    "    However, since the variance of the bootstrap sample has been increased by adding the normal error term, the data are usually rescaled to have the same sample variance as the original observations. So, we replace the equation above in the algorithm with \n",
    "    \n",
    "    \\begin{align}\n",
    "        X_i^{*(b)} & = \\bar{Z}^{*(b)} + (1 + h_k^2/\\hat{\\sigma}^2)^{-1/2} (Z_i^{*(b)} - \\bar{Z}^{*(b)} + h_k\\epsilon_i^{(b)}).\n",
    "    \\end{align}\n",
    "    \n",
    "    We'll use this more complicated variance scaling in the code. In the equation above we have:\n",
    "    - $\\bar{Z}^{*(b)}$ is the sample mean of the bootstrap samples $Z^{*(b)}$.\n",
    "    - $\\hat\\sigma$ is the variance of the original observed data.\n",
    "    - $h_k$ is the minimum bandwidth h such that the density estimate for the original observed data has k modes.\n",
    "    - $\\epsilon_i^{(b)} \\sim \\mathcal{N}(0, 1)$, iid Gaussian noise\n",
    "    \n",
    "   We will call $ X_i^{*(b)}$ **bootstrap replicates**. \n",
    "       \n",
    "       \n",
    "       \n",
    "2. For each bootstrap replicate $X^{*{b}}$, evaluate the value of the test statistic $H_k^{*(b)}$.\n",
    "    \n",
    "3. Estimate the $p$-value as the fraction of time that the value of the test statistic evaluated on the bootstrap replicates is larger the the test statistic evaluated on the original observed data.\n",
    "    \\begin{align}\n",
    "        \\text{estimate of }  \\mathbb{P}_0(H_k \\geq h_k) = \\frac{1}{B} \\sum_{b = 1}^B 1[H_k^{*(b)} \\geq h_k].\n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_p_value(X, B, k=1):\n",
    "    \"\"\"Estimates the p-value for the hypothesis test.\n",
    "    \n",
    "    Inputs: \n",
    "      X: array of floats containing the observed galaxy velocities.\n",
    "      B: int, number of bootstrap samples to draw.\n",
    "      k: int, number of modes we are testing for.\n",
    "    \n",
    "    Outputs:\n",
    "      p_value: float, an estimate of the p-value.\n",
    "    \"\"\"\n",
    "    # Find hk for the distribution under the null hypothesis.\n",
    "    x_values = np.arange(5,35,0.05)\n",
    "    hk = find_hk(x_values, X, k=k)\n",
    "    # Count of the number of times Hk >= hk.\n",
    "    Hk_greater_count = 0\n",
    "    # Variance of the observed data X for rescaling the data.\n",
    "    X_var = np.var(X)\n",
    "    n = len(X)\n",
    "    for _ in range(B):\n",
    "        # TODO: obtain the bootstrap sample Z*. \n",
    "        # Z_star should be an array of n samples drawn from the data array X, sampled with replacement.\n",
    "        # Hint: use np.random.choice.\n",
    "        Z_star = ... # TODO: fill in\n",
    "        \n",
    "        Z_bar = ... # TODO: fill in\n",
    "        epsilon = np.random.normal(size=n)\n",
    "        X_star = ... # TODO: fill in\n",
    "        \n",
    "        # Check if H1 >= h1. Instead of explicitly calculating H1 (which could take long), \n",
    "        # we are using a shortcut where we count the number of modes in X_star under bandwidth value h1.\n",
    "        # If the counted number of modes is greater than the number of modes used to find h1 \n",
    "        # for the observed data, then the bandwidth value H1 is greater than or equal to the bandwidth value h1.\n",
    "        # This is true because of number of modes is monotonically decreasing in the bandwidth value h.\n",
    "        modes = count_modes(x_values, hk, X_star)\n",
    "        if modes > k:\n",
    "            Hk_greater_count += 1\n",
    "    p_value = Hk_greater_count / B \n",
    "    return(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f86a4a",
   "metadata": {},
   "source": [
    "## Try testing for different numbers of modes.\n",
    "\n",
    "If we reject the hypothesis that the distribution of the data has 1 mode, what about testing if the distribution has more than $k$ modes? We can apply the same techniques to test \n",
    "\n",
    "$$H_0: m(p) = k$$ \n",
    "$$H_A: m(p) > k$$ \n",
    "\n",
    "Below, we apply the same techniques to estimate the $p$-values for $k = 2$ and $k = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a35787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, run this cell to calculate the p-value. \n",
    "p_val_1 = estimate_p_value(X_observed, 100, k=1)\n",
    "print(\"p-value for test for more than 1 mode:\", p_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f6e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, run this cell to calculate the p-value.\n",
    "# k = 2\n",
    "p_val_2 = estimate_p_value(X_observed, 100, k=2)\n",
    "print(\"p-value for test for more than 2 modes:\", p_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3abfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, run this cell to calculate the p-value.\n",
    "# k = 3\n",
    "p_val_3 = estimate_p_value(X_observed, 100, k=3)\n",
    "print(\"p-value for test for more than 3 modes:\", p_val_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb081e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests: Do not modify\n",
    "assert(np.abs(p_val_1) <= 0.01)\n",
    "assert(np.abs(p_val_2) <= 0.03)\n",
    "assert(np.abs(p_val_3 - 0.46) <= 0.15)\n",
    "print('Test Passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ed18d",
   "metadata": {},
   "source": [
    "### 4.b. For which values of $k$ were you able to reject the null hypothesis? Did this match your expectation of the number of modes in the data based on looking at the initial histogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66753ad5",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Great job! You've made it to the end of the lab!\")\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('chinchilla.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.axes.get_xaxis().set_visible(False)\n",
    "imgplot.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c15a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
