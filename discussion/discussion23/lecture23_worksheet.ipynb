{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disturbed-price",
   "metadata": {},
   "source": [
    "# Lecture worksheet 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-frederick",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-convertible",
   "metadata": {},
   "source": [
    "### Q1.1 True/False\n",
    "\n",
    "(a) In reinforcement learning, the rewards must be the same for all states.\n",
    "\n",
    "(b) Q-learning is used when the transition probabilities are unknown.\n",
    "\n",
    "\n",
    "### Q1.2 Fill-in-the-blank and short-answer\n",
    "\n",
    "(a) (*Choose one of s, a, or s' to fill in both blanks*) T(s, a, s') defines a probability distribution over ______ (in other words, if     we fix the other two inputs and sum across all values of ______, T should sum   to 1)\n",
    "\n",
    "(b) What is the benefit of using the dynamic programming algorithm for value iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-integrity",
   "metadata": {},
   "source": [
    "## Question 2: Q-iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-consultancy",
   "metadata": {},
   "source": [
    "The following cell contains the code used in lecture for value iteration. Modify it to also save the best value of the Q-function $Q(s, a)$ in addition to the value function $V^*(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined as part of the setup\n",
    "states = [...]  # all states\n",
    "actions = [...]  # all actions\n",
    "γ = 0.9  # Discount factor\n",
    "T = 1000000  # max time\n",
    "def T(s, a, s_new):\n",
    "    \"\"\"\n",
    "    Probability of ending in state s_new when starting in state s and\n",
    "    taking action a\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def R(s, a, s_new):\n",
    "    \"\"\"\n",
    "    Reward for going from state s to state s_new by taking action a\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "## FAST VERSION\n",
    "V_arr = np.zeros([len(states), T])\n",
    "for t in range(T-1, -1, -1):\n",
    "    for s in states:\n",
    "        best_so_far = -np.inf\n",
    "        for a in actions:\n",
    "            q = 0  # Q(s, a)\n",
    "            for s_new in states:\n",
    "                q += (\n",
    "                    T(s, a, s_new) * \n",
    "                    (R(s, a, s_new) + γ * V_arr[s_new, t+1])\n",
    "                )\n",
    "            best_so_far = max(best_so_far, q)\n",
    "        V_arr[s, t] = best_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-shore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "treated-logic",
   "metadata": {},
   "source": [
    "## Q3: GridWorld\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "municipal-perth",
   "metadata": {},
   "source": [
    "Consider the following GridWorld environment:\n",
    "\n",
    "![](grid_world.png)\n",
    "\n",
    "where `start` represents the initial state, $\\times$ represents an inaccessible state (like the example in lecture), and the $-1$ and $100$ states are terminal states with corresponding rewards. All other states have a reward of $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-stanley",
   "metadata": {},
   "source": [
    "### Q3.1\n",
    "\n",
    "Assume state transitions are deterministic. In other words, if our action is to move in a particular direction, we always move in that direction; unless there is a wall, in which case we stay in that same state.\n",
    "\n",
    "If $\\gamma = 0.9$, compute the optimal value $V^*$ for each state.\n",
    "\n",
    "\n",
    "### Q3.2\n",
    "\n",
    "Compute the optimal Q-function at the `start` state for each of the four actions. Based on your answer, what would the optimal policy be for this state (in other words, what is $\\pi($ `start` $)$?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
