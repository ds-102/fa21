{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "future-plumbing",
   "metadata": {},
   "source": [
    "# Lecture worksheet 23 solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-tolerance",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-tender",
   "metadata": {},
   "source": [
    "### Q1.1 True/False\n",
    "\n",
    "(a) In reinforcement learning, the rewards must be the same for all states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-french",
   "metadata": {},
   "source": [
    "**SOLUTION**: False: we set the rewards higher for states that we want to get to, and lower for states we want to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-evanescence",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "(b) Q-learning is used when the transition probabilities are unknown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-operator",
   "metadata": {},
   "source": [
    "**SOLUTION**: True: Q-learning is used when we don't know the transition probabilities and instead only observe trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-connectivity",
   "metadata": {},
   "source": [
    "\n",
    "### Q1.2 Fill-in-the-blank and short-answer\n",
    "\n",
    "(a) (*Choose one of s, a, or s' to fill in both blanks*) T(s, a, s') defines a probability distribution over ______ (in other words, if     we fix the other two inputs and sum across all values of ______, T should sum   to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-riverside",
   "metadata": {},
   "source": [
    "**SOLUTION**: $s'$: $T$ is a conditional distribution over new states conditioned on previous state and action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-agriculture",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "(b) What is the benefit of using the dynamic programming algorithm for value iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-worst",
   "metadata": {},
   "source": [
    "**SOLUTION**: The dynamic programming algorithm is faster (more efficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-operator",
   "metadata": {},
   "source": [
    "## Question 2: Q-iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-characteristic",
   "metadata": {},
   "source": [
    "The following cell contains the code used in lecture for value iteration. Modify it to also save the best value of the Q-function $Q(s, a)$ in addition to the value function $V^*(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-cambridge",
   "metadata": {},
   "source": [
    "**SOLUTION**: Along with `V_arr`, we need to keep track of an array with the Q-values. We need an additional dimension, to keep track of the action as well as the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined as part of the setup\n",
    "states = [...]  # all states\n",
    "actions = [...]  # all actions\n",
    "γ = 0.9  # Discount factor\n",
    "T = 1000000  # max time\n",
    "def T(s, a, s_new):\n",
    "    \"\"\"\n",
    "    Probability of ending in state s_new when starting in state s and\n",
    "    taking action a\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def R(s, a, s_new):\n",
    "    \"\"\"\n",
    "    Reward for going from state s to state s_new by taking action a\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "## FAST VERSION\n",
    "V_arr = np.zeros([len(states), T])\n",
    "### SOLUTION: ADDED LINE\n",
    "Q_arr = np.zeros([len(states), len(actions), T])\n",
    "\n",
    "for t in range(T-1, -1, -1):\n",
    "    for s in states:\n",
    "        best_so_far = -np.inf\n",
    "        for a in actions:\n",
    "            q = 0  # Q(s, a)\n",
    "            for s_new in states:\n",
    "                q += (\n",
    "                    T(s, a, s_new) * \n",
    "                    (R(s, a, s_new) + γ * V_arr[s_new, t+1])\n",
    "                )\n",
    "            ### SOLUTION: ADDED LINE\n",
    "            Q_arr[s, a, t] = q\n",
    "            best_so_far = max(best_so_far, q)\n",
    "        V_arr[s, t] = best_so_far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-compensation",
   "metadata": {},
   "source": [
    "## Q3: GridWorld\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-qualification",
   "metadata": {},
   "source": [
    "Consider the following GridWorld environment:\n",
    "\n",
    "![](grid_world.png)\n",
    "\n",
    "where `start` represents the initial state, $\\times$ represents an inaccessible state (like the example in lecture), and the $-1$ and $100$ states are terminal states with corresponding rewards. All other states have a reward of $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-order",
   "metadata": {},
   "source": [
    "### Q3.1\n",
    "\n",
    "Assume state transitions are deterministic. In other words, if our action is to move in a particular direction, we always move in that direction; unless there is a wall, in which case we stay in that same state.\n",
    "\n",
    "If $\\gamma = 0.9$, compute the optimal value $V^*$ for each state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-corner",
   "metadata": {},
   "source": [
    "**SOLUTION**: \n",
    "\n",
    "1. The two terminal states have undefined $V^*$ values, since there are no actions that can be taken from those states. \n",
    "2. The computation is easiest to do if we start with the state right next to the goal (reward=1) state. We know that the best possible expected sum of rewards comes from going right to the goal state. In this case, the reward for the transition is 1, and the future reward is 0 (since the process ends at a terminal state). So, the optimal value of that state is 1.\n",
    "3. Next, we can do the two states next to it: for each of those, the optimal action is to move toward the goal. The reward for that transition is 0, and the value of that new state (the one next to the goal) is just as we computed in step 2 above, 1. So, we multiply that by our discount factor 0.9 to obtain a value of 0.9.\n",
    "4. For the states next to the ones we just did, we can follow a similar process to obtain values of $0.9^2$, and then $0.9^3$ for the states next to those, and then finally $0.9^4$ for the state in the bottom left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-segment",
   "metadata": {},
   "source": [
    "\n",
    "### Q3.2\n",
    "\n",
    "Compute the optimal Q-function at the `start` state for each of the four actions. Based on your answer, what would the optimal policy be for this state (in other words, what is $\\pi($ `start` $)$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-pacific",
   "metadata": {},
   "source": [
    "**SOLUTION**:\n",
    "\n",
    "Recall the definition of $Q$:\n",
    "$$\n",
    "Q(s, a) = \\sum_{s'} T(s, a, s') \\left[R(s, a, s') + \\gamma V^*(s')\\right]\n",
    "$$\n",
    "\n",
    "In this case:\n",
    "* the transition probabilities are all 1 or 0, which means we only have one term when computing the sum above. \n",
    "* For all actions other than `right`, the reward is 0, and for `right`, the reward is -100.\n",
    "* The value of the next state $s'$ is as we computed above in Q3.1.\n",
    "\n",
    "Using this information, we can compute the Q-values as follows:\n",
    "* Q(`start`, `right`) = $-100 + 0 = -100$\n",
    "* Q(`start`, `up`) = $0 + \\gamma \\times 1 = 0.9$\n",
    "* Q(`start`, `left`) = $0 + \\gamma \\times 0.9 = 0.9^2$\n",
    "* Q(`start`, `down`) = $0 + \\gamma \\times 0.9^2 = 0.9^3$\n",
    "\n",
    "The optimal action (i.e., the one corresponding to the largest value out of these four) is `up`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
