{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture worksheet 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Only problem 1 is graded for credit, all other problems are optional (but still recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) (T/F) For fixed $m$, the regret of ETC grows linearly in the time horizon $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b) (T/F) If we are allowed to choose $m$ based on $\\Delta_a$ and $n$, then ETC can achieve regret that is logarithmic in $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c) (T/F) UCB requires us to know $m$ and $\\Delta_a$ in order to achieve logarithmic regret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1d) In UCB, what value do we set the confidence level $\\delta_t$ to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e) What parameters do we need in order to set up the TS algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1f) (T/F) TS always has lower regret than UCB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1g) Explain what is the exploration-exploitation trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: More on Thompson sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we try to develop more intuition for what Thompson sampling is doing, and show that it also encourages optimistic exploration, like UCB. Suppose each prior $\\pi_a$ is a Beta(2,2) random variable, and the likelihood $p_a(x|\\mu_a)$ is a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) What is the distribution for the posterior $P_{a,t}$ for arm $a$? Write your answer in terms of the rewards observed for arm $a$: $X_{1,a},\\ldots,X_{T_a(t-1),a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b) Consider some arm $a$. Keeping the rewards observed for all other arms fixed, explain why we are more likely to pull arm $a$ if $\\hat{\\mu}_a(t-1)$ is large or if $T_a(t-1)$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Modeling with bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are working for a technology company that wants to test the effect of a change in their app interface on user engagement. How would you model this as a bandits problem? What are the limitations of doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Reward distributions that are not sub-Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCB requires sub-Gaussian rewards in order to have a logarithmic regret bound. Can you think of a simple change to the algorithm so that it has logarithmic regret for any reward distribution with known finite variance $\\sigma$?\n",
    "\n",
    "*Hint: Consider problem 2 on HW 5.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Lower bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that for any time horizon $n$, and any number of arms $K$, there is a bandit environment (with bounded rewards) for which the regret of any algorithm is at least $\\Omega(\\sqrt{nK})$. Why does this not contradict the logarithmic regret bounds for UCB and TS? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
