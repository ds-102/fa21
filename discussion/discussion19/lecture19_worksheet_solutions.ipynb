{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture worksheet 19 solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Only problem 1 is graded for credit, all other problems are optional (but still recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) (T/F) For fixed $m$, the regret of ETC grows linearly in the time horizon $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b) (T/F) If we are allowed to choose $m$ based on $\\Delta_a$ and $n$, then ETC can achieve regret that is logarithmic in $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c) (T/F) UCB requires us to know $m$ and $\\Delta_a$ in order to achieve logarithmic regret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1d) In UCB, what value do we set the confidence level $\\delta_t$ to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** $\\delta_t = 1/t^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e) What parameters do we need in order to set up the TS algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We need to specify priors for the arm means and likelihoods for the rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1f) (T/F) TS always has lower regret than UCB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1g) Explain what is the exploration-exploitation trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We want to make the best choice given current knowledge (exploitation), and yet on the other hand, we want to make diverse choices in order to acquire more information with which we can improve future decisions (exploration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: More on Thompson sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we try to develop more intuition for what Thompson sampling is doing, and show that it also encourages optimistic exploration, like UCB. Suppose each prior $\\pi_a$ is a Beta(2,2) random variable, and the likelihood $p_a(x|\\mu_a)$ is a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) What is the distribution for the posterior $P_{a,t}$ for arm $a$? Write your answer in terms of the rewards observed for arm $a$: $X_{1,a},\\ldots,X_{T_a(t-1),a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Since the number of ones observed is $S = \\sum_{i=1}^{T_a(t-1)} X_{i,a}$, and the number of zeros is $T_a(t-1) - S$, the posterior is $\\text{Beta}(2+S,T_a(t-1)+2-S)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b) Consider some arm $a$. Keeping the rewards observed for all other arms fixed, explain why we are more likely to pull arm $a$ if $\\hat{\\mu}_a(t-1)$ is large or if $T_a(t-1)$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** The posterior mean is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bar\\mu_{a,t} & = \\frac{2+S}{(2+S) + (T_a(t-1)+2-S)} \\\\\n",
    "& = \\frac{2+S}{T_a(t-1)+4} \\\\\n",
    "& = \\frac{T_a(t-1)}{T_a(t-1)+4}\\hat{\\mu}_a(t-1) + \\frac{2}{T_a(t-1)+4} \\\\\n",
    "& \\approx \\hat{\\mu}_a(t-1).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence, the larger $\\mu_a(t-1)$, the larger the posterior mean. Furthermore, setting $a = 2+S$, $b = T_a(t-1)+2-S$, the variance is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{ab}{(a+b)^2(a+b+1)} & = \\frac{a}{a+b} \\cdot \\frac{b}{a+b} \\cdot \\frac{1}{a+b+1} \\\\\n",
    "& = \\bar\\mu_{a,t}(1-\\bar\\mu_{a,t})\\cdot \\frac{1}{T_a(t-1) + 4}.\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, we see that the smaller $T_a(t-1)$ is, the larger the variance. In either case, this implies that $\\mu_{a,t}$ has a higher probability of being bigger than other arms (and of hence being selected by Thompson sampling).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Modeling with bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are working for a technology company that wants to test the effect of a change in their app interface on user engagement. How would you model this as a bandits problem? What are the limitations of doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We have two arms:\n",
    "- Arm 1: the old interface\n",
    "- Arm 2: the new interface\n",
    "\n",
    "Every round represents a user. Showing the user the old interface corresponds to pulling Arm 1. Showing them the new interface corresponds to pulling Arm 2. The reward is the amount of engagement for that user.\n",
    "\n",
    "Limitations: We have to wait a long time before seeing the engagement for each user. If we use the traditional bandit framework, then we have to observe this before making a decision for the next user, which makes everything very slow. Instead, it's more natural to pull arms in parallel for many users at the same time, and update our UCB / posteriors for the arms in batches.\n",
    "\n",
    "Some interesting discussion can be found [here](https://cxl.com/blog/bandit-tests/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Reward distributions that are not sub-Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCB requires sub-Gaussian rewards in order to have a logarithmic regret bound. Can you think of a simple change to the algorithm so that it has logarithmic regret for any reward distribution with known finite variance $\\sigma$?\n",
    "\n",
    "*Hint: Consider problem 2 on HW 5.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We make use of the median-of-means estimator to get an estimate $\\hat \\mu_{Med,a}(t)$ for the mean of each arm, and make use of the concentration inequality derived in HW5 to get an appropriate upper confidence bound for the true mean $\\mu_a$ of that arm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Lower bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that for any time horizon $n$, and any number of arms $K$, there is a bandit environment (with bounded rewards) for which the regret of any algorithm is at least $\\Omega(\\sqrt{nK})$. Why does this not contradict the logarithmic regret bounds for UCB and TS? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** The regret bound for UCB (for instance) is\n",
    "$$\n",
    "    R_n \\leq 3\\sum_{a=1}^K \\Delta_a + \\sum_{a : \\Delta_a > 0} \\frac{24\\log(n)}{\\Delta_a}\n",
    "$$\n",
    "It depends on the suboptimality gaps $\\Delta_a$, and becomes very large when $\\Delta_a$ is small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
