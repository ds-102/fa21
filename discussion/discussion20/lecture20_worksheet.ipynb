{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "selective-saudi",
   "metadata": {},
   "source": [
    "# Lecture worksheet 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-hollow",
   "metadata": {},
   "source": [
    "#### Reminder: Only question 1 will be graded for credit. All other problems are optional (but still recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-graphic",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-animal",
   "metadata": {},
   "source": [
    "### Q1.1 The Two Cultures\n",
    "\n",
    "What did you think of the assigned reading? Did you find Breiman's arguments in favor of \"data modeling\" persuasive? If you read any of the responses, did you find them persuasive?\n",
    "\n",
    "*Your answer should be at least 2 sentences long.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-selection",
   "metadata": {},
   "source": [
    "### Q1.2 True/False\n",
    "\n",
    "For each of the following, specify whether it's true or false. \n",
    "\n",
    "*Optional: If it's False, explain why.*\n",
    "\n",
    "(a) All nonparametric models are hard to interpret.\n",
    "\n",
    "(b) Nonparametric methods are always faster than parametric methods.\n",
    "\n",
    "(c) The term \"nonparametric\" can mean either: methods that don't make assumptions about the distribution of the data/parameters; **_or_** methods where the number of parameters (e.g., coefficients) is infinite or increases with the number of data points.\n",
    "\n",
    "(d) Decision trees can only be used for classification.\n",
    "\n",
    "(e) Random forests use a collection of decision trees where each tree is trained on fewer data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-knife",
   "metadata": {},
   "source": [
    "## Question 2: Implementing (part of) a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-guest",
   "metadata": {},
   "source": [
    "Fill in the blanks in the following code that implements a random forest for regression, based on what was discussed in lecture. Assume the rest of the code is already written (including the parts that set up the instance variables, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class RandomForestRegressor:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # A list of all the trees in the forest\n",
    "        self.trees = []\n",
    "        \n",
    "        # Loop over the trees in the forest\n",
    "        for i in self.n_estimators:\n",
    "            tree = DecisionTreeRegressor(...) # No need to fill this in\n",
    "            \n",
    "            rows = ... # TODO: FILL ME IN\n",
    "            columns = ... # TODO: FILL ME IN\n",
    "            X_for_tree = ... # TODO: FILL ME IN\n",
    "            y_for_tree = ... # TODO: FILL ME IN\n",
    "            \n",
    "            tree.fit(X_for_tree, y_for_tree)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Array that will hold the sum of the predictions from all trees\n",
    "        prediction_sum = np.zeros(X.shape[0])\n",
    "        \n",
    "        # Loop over the trees in the forest\n",
    "        for tree in self.trees:\n",
    "            # TODO: FILL IN THE REST OF THE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-shuttle",
   "metadata": {},
   "source": [
    "## Question 3: Trees, Forests, Bias, and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-present",
   "metadata": {},
   "source": [
    "Recall from much earlier in the semester that we can write the expected squared loss (i.e., the risk) of a decision $\\delta(x)$ and a true value of the parameter $\\theta$ as\n",
    "\n",
    "$$\n",
    "E[(\\delta(x) - \\theta)^2] = \\underbrace{E\\left[\\left(\\delta(x) - E[\\delta(x)]\\right)^2\\right]}_{\\text{variance of } \\delta(x)} + \\underbrace{(E[\\delta(x)] - \\theta)^2}_{\\text{bias}^2}\n",
    "$$\n",
    "\n",
    "If our decision is a prediction for $y$ that we call $\\hat{y}$, then $\\theta = y$ and $\\delta(x) = \\hat{y}(x)$:\n",
    "\n",
    "$$\n",
    "E[(\\hat{y}(x) - y)^2] = \\underbrace{E\\left[\\left(\\hat{y}(x) - E[\\hat{y}(x)]\\right)^2\\right]}_{\\text{variance of prediction } \\hat{y}(x)} + \\underbrace{(E[\\delta(x)] - y)^2}_{\\text{bias}^2}\n",
    "$$\n",
    "\n",
    "Fill in each blank in the following statement with either \"bias\" or \"variance\", and explain:\n",
    "\n",
    "**Decision trees have high ____________, and low ____________. By averaging them in a random forest, we lower the ____________.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
