{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "typical-example",
   "metadata": {},
   "source": [
    "# Lecture worksheet 20 solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-teddy",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-annual",
   "metadata": {},
   "source": [
    "### Q1.1 The Two Cultures\n",
    "\n",
    "What did you think of the assigned reading? Did you find Breiman's arguments in favor of \"algorithmic modeling\" persuasive? If you read any of the responses, did you find them persuasive?\n",
    "\n",
    "*Please see the discussion video recording for Ramesh and Yan Shuo's thoughts on the paper.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-garage",
   "metadata": {},
   "source": [
    "### Q1.2 True/False\n",
    "\n",
    "For each of the following, specify whether it's true or false. \n",
    "\n",
    "*Optional: If it's False, explain why.*\n",
    "\n",
    "(a) All nonparametric models are hard to interpret.\n",
    "\n",
    "**False: some nonparametrics (e.g., small decision trees) are very easy to interpret.**\n",
    "\n",
    "(b) Nonparametric methods are always faster than parametric methods.\n",
    "\n",
    "**False: both parametric methods and nonparametric methods can be slow. For example, a large, complex (parametric) Bayesian model could be much slower than a simple (nonparametric) decision tree. On the other hand, (parametric) linear regression could be much faster than fitting a large (nonparametric) random forest.**\n",
    "\n",
    "(c) The term \"nonparametric\" can mean either: methods that don't make assumptions about the distribution of the data/parameters; **_or_** methods where the number of parameters (e.g., coefficients) is infinite or increases with the number of data points.\n",
    "\n",
    "**True**\n",
    "\n",
    "(d) Decision trees can only be used for classification.\n",
    "\n",
    "**False: decision trees can be used for regression or classification.**\n",
    "\n",
    "(e) Random forests use a collection of decision trees where each tree is trained on fewer data points.\n",
    "\n",
    "**False: each tree is trained on a _bootstrap sample_ of the original data set, which samples the same number of points with replacement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-redhead",
   "metadata": {},
   "source": [
    "## Question 2: Implementing (part of) a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-envelope",
   "metadata": {},
   "source": [
    "Fill in the blanks in the following code that implements a random forest for regression, based on what was discussed in lecture. Assume the rest of the code is already written (including the parts that set up the instance variables, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class RandomForestRegressor:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fits the model\n",
    "        \n",
    "        X: array, num_pts x num_features\n",
    "        y: array, num_pts\n",
    "        \"\"\"\n",
    "        # A list of all the trees in the forest\n",
    "        self.trees = []\n",
    "        \n",
    "        # Loop over the trees in the forest\n",
    "        for i in self.n_estimators:\n",
    "            tree = DecisionTreeRegressor(...) # No need to fill this in\n",
    "            \n",
    "            ### SOLUTION\n",
    "            N, K = X.shape\n",
    "            \n",
    "            data_indices = np.arange(N)\n",
    "            feature_indices = np.arange(K)\n",
    "            \n",
    "            rows = np.random.choice(indices, N, replace=True)\n",
    "            columns = np.random.choice(feature_indices, K//3, replace=False)\n",
    "            \n",
    "            X_for_tree = X[rows, columns] \n",
    "            y_for_tree = y[rows] \n",
    "            ### END SOLUTION\n",
    "            \n",
    "            tree.fit(X_for_tree, y_for_tree)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Array that will hold the sum of the predictions from all trees\n",
    "        prediction_sum = np.zeros(X.shape[0])\n",
    "        \n",
    "        # Loop over the trees in the forest\n",
    "        for tree in self.trees:\n",
    "        ### SOLUTION\n",
    "            one_tree_prediction = tree.predict(X)\n",
    "            prediction_sum += one_tree_prediction\n",
    "        return prediction_sum / len(self.trees)\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-maria",
   "metadata": {},
   "source": [
    "## Question 3: Trees, Forests, Bias, and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-symphony",
   "metadata": {},
   "source": [
    "Recall from much earlier in the semester that we can write the expected squared loss (i.e., the risk) of a decision $\\delta(x)$ and a true value of the parameter $\\theta$ as\n",
    "\n",
    "$$\n",
    "E[(\\delta(x) - \\theta)^2] = \\underbrace{E\\left[\\left(\\delta(x) - E[\\delta(x)]\\right)^2\\right]}_{\\text{variance of } \\delta(x)} + \\underbrace{(E[\\delta(x)] - \\theta)^2}_{\\text{bias}^2}\n",
    "$$\n",
    "\n",
    "If our decision is a prediction for $y$ that we call $\\hat{y}$, then $\\theta = y$ and $\\delta(x) = \\hat{y}(x)$:\n",
    "\n",
    "$$\n",
    "E[(\\hat{y}(x) - y)^2] = \\underbrace{E\\left[\\left(\\hat{y}(x) - E[\\hat{y}(x)]\\right)^2\\right]}_{\\text{variance of prediction } \\hat{y}(x)} + \\underbrace{(E[\\delta(x)] - y)^2}_{\\text{bias}^2}\n",
    "$$\n",
    "\n",
    "Fill in each blank in the following statement with either \"bias\" or \"variance\", and explain:\n",
    "\n",
    "**Decision trees (with no limit on depth) have high __Variance__, and low __bias__. By averaging them in a random forest, we lower the variance.**\n",
    "\n",
    "#### SOLUTION\n",
    "\n",
    "* Decision trees have high variance: as the data shift, the possible tree could be very different.\n",
    "* Decision trees have low bias: the \"average decision tree\" will end up pretty close to the true parameter. This is because decision trees can capture complex structure in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-render",
   "metadata": {},
   "source": [
    "Here's an example that illustrates that. The first two cells are duplicated from the lecture notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell is exactly the same as the version from the notes\n",
    "N_test = 500\n",
    "N_train = 150\n",
    "\n",
    "np.random.seed(2021)\n",
    "\n",
    "# Create a training dataset\n",
    "x1_train = np.random.uniform(-1, 1, N_train)\n",
    "x2_train = np.random.uniform(-1, 1, N_train)\n",
    "\n",
    "y_train = (x1_train * x2_train > 0).astype(np.int64)\n",
    "\n",
    "\n",
    "# Create a feature matrix that we can use for classification\n",
    "X_train = np.vstack([x1_train, x2_train]).transpose()\n",
    "\n",
    "\n",
    "# Create a test dataset\n",
    "x1_test = np.random.uniform(-3, 3, N_test)\n",
    "x2_test = np.random.uniform(-3, 3, N_test)\n",
    "\n",
    "y_test = (x1_test * x2_test > 0).astype(np.int64)\n",
    "\n",
    "\n",
    "# Create a feature matrix that we can use to evaluate\n",
    "X_test = np.vstack([x1_test, x2_test]).transpose()\n",
    "\n",
    "def draw_results(x1, x2, color, plot_title=''):\n",
    "    plt.figure()\n",
    "    plt.scatter(x1, x2, c=color, cmap='viridis', alpha=0.7);\n",
    "    plt.colorbar()\n",
    "    plt.title(plot_title)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "draw_results(\n",
    "    x1_train, x2_train, color=y_train,\n",
    "    plot_title=\"Training data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-composition",
   "metadata": {},
   "source": [
    "This cell randomly flips a few of the $y$-values in the training dataset and then trains a decision tree. Try running the cell several times and look at the results. The `flip_p` parameter, which controls what proportion of the $y$-values get flipped, is initially 0. \n",
    "\n",
    "Try running the cell a few times with `flip_p = 0`. How well does the decision tree do when there's no noise in the training dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-softball",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flip_p = 0.0  # probability of flipping y for training\n",
    "\n",
    "flipped_y = y_train.copy()\n",
    "flips = np.random.random(N_train) < flip_p\n",
    "flipped_y[flips] = 1 - flipped_y[flips]\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, flipped_y)\n",
    "\n",
    "probs = tree.predict_proba(X_test)[:, 1]\n",
    "y_hat = (probs > 0.5).astype(np.int64)\n",
    "\n",
    "draw_results(\n",
    "    x1_train, x2_train, color=flipped_y,\n",
    "    plot_title=\"Training data (with noise)\"\n",
    ")\n",
    "\n",
    "draw_results(\n",
    "    x1_test, x2_test, color=probs, \n",
    "    plot_title=\"Predicted probability of y=1\"\n",
    ")\n",
    "\n",
    "accuracy = np.mean(y_test == y_hat)\n",
    "print(f\"Accuracy on test set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-civilian",
   "metadata": {},
   "source": [
    "Now, try setting `flip_p` to 0.1 and see how the results change. As the data $x$ vary, you should observe a lot of variance in the predictions $\\hat{y}(x)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
